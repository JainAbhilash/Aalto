{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a7979e8caaaed8a717e326be94fd4f6",
     "grade": false,
     "grade_id": "cell-440df6cfa709812f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4. Convolutional networks\n",
    "\n",
    "## Part 2. VGG-style network\n",
    "\n",
    "In the second part you need to train a convolutional neural network with an architecture inspired by a VGG-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5dbff9bac5479ad10101fdef6a7c2a7",
     "grade": false,
     "grade_id": "cell-902667dea4d68204",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# Select data directory\n",
    "import os\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    # course_data_dir = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e0d4635b5905e98963ce92fbfb03375",
     "grade": false,
     "grade_id": "cell-d1a0f2d597ac9692",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48d33ffe246f5459117f53cac15b370d",
     "grade": false,
     "grade_id": "cell-fe95dcf02c6b9c5e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f2b11aa8f0d0377563333bd78493751",
     "grade": false,
     "grade_id": "cell-e5b565cc4aae8e7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## FashionMNIST dataset\n",
    "\n",
    "Let us use the FashionMNIST dataset. It consists of 60,000 training images of 10 classes: 'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05dae4d3d681cf9666b90ac940628a83",
     "grade": false,
     "grade_id": "cell-8b0fded08998282c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in /coursedata/fashion_mnist\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Min-max scaling to [-1, 1]\n",
    "])\n",
    "\n",
    "data_dir = os.path.join(course_data_dir, 'fashion_mnist')\n",
    "print('Data stored in %s' % data_dir)\n",
    "trainset = torchvision.datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "           'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c554ba5450d41d232bee8a726dca64aa",
     "grade": false,
     "grade_id": "cell-b107418d64770e19",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This function computes the accuracy on the test dataset\n",
    "def compute_accuracy(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eccfe8c399d71b5018afc8ef3b0e9132",
     "grade": false,
     "grade_id": "cell-4ab9d042e4edcd54",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# VGG-style network\n",
    "\n",
    "Let us now define a convolution neural network with an architecture inspired by the [VGG-net](https://arxiv.org/abs/1409.1556):\n",
    "\n",
    "<img src=\"vgg-style.png\" width=600 style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7131564b1434160a11e549f27efed8f1",
     "grade": false,
     "grade_id": "cell-91295b0ab2098018",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The architecture:\n",
    "* A block of three convolutional layers with:\n",
    "    * 3x3 kernel\n",
    "    * 16 output channels\n",
    "    * one pixel zero-pading on both sides\n",
    "    * 2d batch normalization after each convolutional layer\n",
    "    * ReLU nonlinearity after each 2d batch normalization layer\n",
    "* Max pooling layer with 2x2 kernel and stride 2.\n",
    "* A block of three convolutional layers with:\n",
    "    * 3x3 kernel\n",
    "    * 32 output channels\n",
    "    * one pixel zero-pading on both sides\n",
    "    * 2d batch normalization after each convolutional layer\n",
    "    * ReLU nonlinearity after each 2d batch normalization layer\n",
    "* Max pooling layer with 2x2 kernel and stride 2.\n",
    "* One convolutional layer with:\n",
    "    * 3x3 kernel\n",
    "    * 48 output channels\n",
    "    * *no padding*\n",
    "    * 2d batch normalization after the convolutional layer\n",
    "    * ReLU nonlinearity after the 2d batch normalization layer\n",
    "* One convolutional layer with:\n",
    "    * 1x1 kernel\n",
    "    * 32 output channels\n",
    "    * *no padding*\n",
    "    * 2d batch normalization after the convolutional layer\n",
    "    * ReLU nonlinearity after the 2d batch normalization layer\n",
    "* One convolutional layer with:\n",
    "    * 1x1 kernel\n",
    "    * 16 output channels\n",
    "    * *no padding*\n",
    "    * 2d batch normalization after the convolutional layer\n",
    "    * ReLU nonlinearity after the 2d batch normalization layer\n",
    "* Global average pooling:\n",
    "    * 5x5 kernel (the input of the layer should be 5x5)\n",
    "* A fully-connected layer with 10 outputs (no nonlinearity)\n",
    "\n",
    "**Note: Batch normalization is expected to be right after a convolutional layer, before nonlinearity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d524906ba4a26f1dec8c0e325893d21",
     "grade": false,
     "grade_id": "cell-958d9ce586a51bd3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, n_channels=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_channels (int): Number of channels in the first convolutional layer. The number of channels in the\n",
    "                             following layers are the multipliers of n_channels.\n",
    "        \"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.block1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=n_channels,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=n_channels,out_channels=n_channels,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.block2=nn.Sequential(\n",
    "            nn.Conv2d(n_channels,out_channels=n_channels*2,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(n_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=n_channels*2,out_channels=n_channels*2,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(n_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=n_channels*2,out_channels=n_channels*2,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(n_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.block3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_channels*2,out_channels=n_channels*3,kernel_size=3),\n",
    "            nn.BatchNorm2d(n_channels*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=n_channels*3,out_channels=n_channels*2,kernel_size=1),\n",
    "            nn.BatchNorm2d(n_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=n_channels*2,out_channels=n_channels,kernel_size=1),\n",
    "            nn.BatchNorm2d(n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=5))\n",
    "        self.fc1 = nn.Linear(n_channels, 10)\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        \"\"\"You can (optionally) print the shapes of the intermediate variables with verbose=True.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out=self.fc1(out)\n",
    "        return out\n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: torch.Size([32, 1, 28, 28])\n",
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Let's test the shapes of the tensors\n",
    "net = VGGNet()\n",
    "net.to(device)\n",
    "\n",
    "# Feed a batch of images from the training data to test the network\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.to(device)\n",
    "    print('Shape of the input tensor:', images.shape)\n",
    "\n",
    "    y = net(images, verbose=True)\n",
    "    assert y.shape == torch.Size([32, 10]), \"Bad shape of y: y.shape={}\".format(y.shape)\n",
    "\n",
    "print('The shapes seem to be ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "439bd75312c86cecd1d5af1d8a8b6760",
     "grade": true,
     "grade_id": "architecture_soft",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "375155e105b5dd7a4cf45f2e4b1c5405",
     "grade": true,
     "grade_id": "architecture_hard",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cc9be08ff34d62640091085d6cbce15",
     "grade": false,
     "grade_id": "cell-7e781e592e778a97",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let us train the network using the same training loop\n",
    "net = VGGNet()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6e85b0f9e5de8c74ee0871f927810e4",
     "grade": false,
     "grade_id": "cell-c872a03d9a23c7d8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31143e4e2ef6266917e7181733bd6111",
     "grade": false,
     "grade_id": "training_loop",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 1.047\n",
      "[1,   400] loss: 0.628\n",
      "[1,   600] loss: 0.525\n",
      "[1,   800] loss: 0.473\n",
      "[1,  1000] loss: 0.433\n",
      "[1,  1200] loss: 0.429\n",
      "[1,  1400] loss: 0.412\n",
      "[1,  1600] loss: 0.368\n",
      "[1,  1800] loss: 0.361\n",
      "Accuracy of the network on the 10000 test images: 86 %\n",
      "[2,   200] loss: 0.493\n",
      "[2,   400] loss: 0.398\n",
      "[2,   600] loss: 0.347\n",
      "[2,   800] loss: 0.360\n",
      "[2,  1000] loss: 0.348\n",
      "[2,  1200] loss: 0.351\n",
      "[2,  1400] loss: 0.351\n",
      "[2,  1600] loss: 0.332\n",
      "[2,  1800] loss: 0.325\n",
      "Accuracy of the network on the 10000 test images: 88 %\n",
      "[3,   200] loss: 0.283\n",
      "[3,   400] loss: 0.310\n",
      "[3,   600] loss: 0.319\n",
      "[3,   800] loss: 0.308\n",
      "[3,  1000] loss: 0.314\n",
      "[3,  1200] loss: 0.314\n",
      "[3,  1400] loss: 0.309\n",
      "[3,  1600] loss: 0.293\n",
      "[3,  1800] loss: 0.293\n",
      "Accuracy of the network on the 10000 test images: 89 %\n",
      "[4,   200] loss: 0.304\n",
      "[4,   400] loss: 0.286\n",
      "[4,   600] loss: 0.282\n",
      "[4,   800] loss: 0.287\n",
      "[4,  1000] loss: 0.285\n",
      "[4,  1200] loss: 0.280\n",
      "[4,  1400] loss: 0.270\n",
      "[4,  1600] loss: 0.279\n",
      "[4,  1800] loss: 0.263\n",
      "Accuracy of the network on the 10000 test images: 89 %\n",
      "[5,   200] loss: 0.260\n",
      "[5,   400] loss: 0.258\n",
      "[5,   600] loss: 0.261\n",
      "[5,   800] loss: 0.275\n",
      "[5,  1000] loss: 0.271\n",
      "[5,  1200] loss: 0.264\n",
      "[5,  1400] loss: 0.277\n",
      "[5,  1600] loss: 0.275\n",
      "[5,  1800] loss: 0.250\n",
      "Accuracy of the network on the 10000 test images: 90 %\n",
      "[6,   200] loss: 0.248\n",
      "[6,   400] loss: 0.255\n",
      "[6,   600] loss: 0.253\n",
      "[6,   800] loss: 0.259\n",
      "[6,  1000] loss: 0.260\n",
      "[6,  1200] loss: 0.259\n",
      "[6,  1400] loss: 0.247\n",
      "[6,  1600] loss: 0.262\n",
      "[6,  1800] loss: 0.246\n",
      "Accuracy of the network on the 10000 test images: 88 %\n",
      "[7,   200] loss: 0.249\n",
      "[7,   400] loss: 0.238\n",
      "[7,   600] loss: 0.251\n",
      "[7,   800] loss: 0.246\n",
      "[7,  1000] loss: 0.236\n",
      "[7,  1200] loss: 0.237\n",
      "[7,  1400] loss: 0.252\n",
      "[7,  1600] loss: 0.245\n",
      "[7,  1800] loss: 0.256\n",
      "Accuracy of the network on the 10000 test images: 90 %\n",
      "[8,   200] loss: 0.233\n",
      "[8,   400] loss: 0.234\n",
      "[8,   600] loss: 0.226\n",
      "[8,   800] loss: 0.239\n",
      "[8,  1000] loss: 0.253\n",
      "[8,  1200] loss: 0.244\n",
      "[8,  1400] loss: 0.247\n",
      "[8,  1600] loss: 0.243\n",
      "[8,  1800] loss: 0.241\n",
      "Accuracy of the network on the 10000 test images: 89 %\n",
      "[9,   200] loss: 0.227\n",
      "[9,   400] loss: 0.234\n",
      "[9,   600] loss: 0.243\n",
      "[9,   800] loss: 0.238\n",
      "[9,  1000] loss: 0.230\n",
      "[9,  1200] loss: 0.243\n",
      "[9,  1400] loss: 0.234\n",
      "[9,  1600] loss: 0.230\n",
      "[9,  1800] loss: 0.234\n",
      "Accuracy of the network on the 10000 test images: 89 %\n",
      "[10,   200] loss: 0.220\n",
      "[10,   400] loss: 0.226\n",
      "[10,   600] loss: 0.224\n",
      "[10,   800] loss: 0.231\n",
      "[10,  1000] loss: 0.233\n",
      "[10,  1200] loss: 0.227\n",
      "[10,  1400] loss: 0.280\n",
      "[10,  1600] loss: 0.230\n",
      "[10,  1800] loss: 0.218\n",
      "Accuracy of the network on the 10000 test images: 89 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    print_every = 200  # mini-batches\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        # Transfer to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i % print_every) == (print_every-1):\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "        if skip_training:\n",
    "            break\n",
    "\n",
    "    # Print accuracy after every epoch\n",
    "    accuracy = compute_accuracy(net, testloader)\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * accuracy))\n",
    "\n",
    "    if skip_training:\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac20170c5701923a114407c375033e5f",
     "grade": false,
     "grade_id": "cell-ce7291729e25c18d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You should get the test accuracy slightly better than the accuracy of a simple conv net from part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a2b0fdf37f54595b6395b0946bdcc5b",
     "grade": true,
     "grade_id": "save",
     "locked": true,
     "points": 0.001,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 4_vgg_net.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the network to a file, submit this file together with your notebook\n",
    "filename = '4_vgg_net.pth'\n",
    "if not skip_training:\n",
    "    try:\n",
    "        do_save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            torch.save(net.state_dict(), filename)\n",
    "            print('Model saved to %s' % filename)\n",
    "        else:\n",
    "            print('Model not saved')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=True.')\n",
    "else:\n",
    "    net = VGGNet()\n",
    "    net.load_state_dict(torch.load(filename, map_location=lambda storage, loc: storage))\n",
    "    net.to(device)\n",
    "    print('Model loaded from %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9663db7adfa1142b25e40d1b03003d8d",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the VGG net on the test images: 0.897\n"
     ]
    }
   ],
   "source": [
    "# Let us compute the accuracy on the test set\n",
    "accuracy = compute_accuracy(net, testloader)\n",
    "print('Accuracy of the VGG net on the test images: %.3f' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
