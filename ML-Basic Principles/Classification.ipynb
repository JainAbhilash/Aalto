{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbb55026603920d0b675b7f0935c923f",
     "grade": false,
     "grade_id": "cell-a5592b3f1c6bf91d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#  Machine Learning: Basic Principles 2018\n",
    "## Classification\n",
    "## Learning goals\n",
    "In this exercise, you will learn how to formulate and solve a classification problem. A classification problem amounts to finding a good classifier which maps a given data point (e.g. a snapshot taken by an on-board camera) via its features to a particular label. The label indicates to which class or category the data point belongs. \n",
    "We will implement **logistic regression** and use **Gradient Descent** to find the optimal weights. We will look at binary as well as multiclass classification. \n",
    "\n",
    "## Exercise Contents\n",
    "\n",
    "1. Introduction\n",
    "    * Here we formulate a classification problem.\n",
    "2. Dataset\n",
    "    * A description of the dataset.\n",
    "3. Exercise\n",
    "    * A total of 5 tasks and 1 demo. Read the task descriptions and answer according to the tasks.\n",
    "        * 3.1 **Getting Hands on the Data**\n",
    "        * 3.2 **Logistic Regression**\n",
    "        * 3.3 **Gradient Descent Step Size**\n",
    "        * 3.4 **Accuracy - How well did we do?**\n",
    "        * 3.5 **Multiclass Classification (One vs All)**\n",
    "\n",
    "### Keywords\n",
    "`Classification`,`Logistic Regression`, `Sigmoid Function`, `Gradient Descent (GD)`\n",
    "\n",
    "## 1 Introduction\n",
    "Suppose you are an intern at the fictive company `Hunda` whose brand-new lawn mower robot uses its on-board camera to find out on which surface it is moving.\n",
    "Your job is to develop a firmware module which allows the mower robot to classify images generated by the camera according to the categories \"I see grass\", \"I see soil\", \"I see tiles\". \n",
    "\n",
    "To do so we will use a specific classification method named logistic regression. We will first develop a logistic regression model which can distinguish between \"I see grass\" and \"I do not see grass\". After that we will extend this model to also be able to distinguish non-grass images further between soil and tiles images. \n",
    "\n",
    "In order to develop this module you are provided with a bunch of snapshots that have been labeled according to these three categories manually by the previous summer intern. Thus, we can use this labeled data to train the firmware module. \n",
    "\n",
    "![](./images/banner.jpg)\n",
    "\n",
    "## 2 Data\n",
    "The available dataset consists of $N=55$ pictures, saved in the folder named `images`:\n",
    "* 20 images of grass (`image_1.jpg` to `image_20.jpg`)\n",
    "* 20 images of soil (`image_21.jpg` to `image_40.jpg`)\n",
    "* 15 images of tiles (`image_41.jpg` to `image_55.jpg`)\n",
    "\n",
    "You can use the Python package `PIL` (=the Python Imaging Library) to access the image data. The basics of PIL are demonstrated below, so you can use those functions in the exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fb1d55d5dde9caac29900cc51923ffb",
     "grade": false,
     "grade_id": "cell-245dfc59a01e2bab",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Read in an image from a jpg-file and store it in the variable im\n",
    "im = Image.open(\"images/image_1.jpg\");\n",
    "\n",
    "# Determine the size of the image\n",
    "width, height = im.size;\n",
    "print('width: %d, height: %d' % (width, height))\n",
    "       \n",
    "\n",
    "# Convert the image to RGB\n",
    "rgb_im = im.convert('RGB');\n",
    "\n",
    "# Determine the rgb values of the pixel at location (row,col) where both row and col are 0\n",
    "pixel = rgb_im.getpixel((0,0))\n",
    "print('Pixel (R, G, B): (%d, %d, %d)' % (pixel[0], pixel[1], pixel[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44945d22d0163999c33635b5a24b4393",
     "grade": false,
     "grade_id": "cell-4a084869afc0c5ec",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3 Exercise\n",
    "The actual exercise starts from here and it has been divided into 5 tasks and 1 demo:\n",
    "* 3.1 **Getting Hands on the Data**\n",
    "* 3.2 **Logistic Regression**\n",
    "* 3.3 **Gradient Descent Step Size**\n",
    "* 3.4 **Accuracy - How well did we do?**\n",
    "* 3.5 **Multiclass Classification (One vs All)**\n",
    "\n",
    "Your task is to fill in `...` under `### STUDENT TASK ###` in each step.\n",
    "\n",
    "## 3.1 Getting Hands on the Data\n",
    "After reading in the RGB images, we have to change them into a form that we can process more easily. Although the images are quite small (less than 3000x3000 pixels) we cannot easily process an image by just stacking the pixels into a vector since it would be of size $3000^2$. Instead, we represent an image by only $d=3$ features, namely the average red, green and blue components (the \"redness\", \"greenness\" and \"blueness\") which are denoted $x_{r}, x_{g}, x_{b}$. Thus, we characterize the $i$th image in our dataset using the feature vector $\\mathbf{x}^{(i)} = \\big(x_{r}^{(i)},x_{g}^{(i)},x_{b}^{(i)} \\big)^{T} \\in \\mathbb{R}^{3}$. In particular, $x^{(1)}_{r}$ denotes the average red component of the first image in the trainging dataset. \n",
    "\n",
    "It will be convenient to stack the feature vectors $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{3}$ obtained for all images in our (training) dataset into the feature matrix \n",
    "\\begin{equation*}\n",
    "    \\mathbf{X} = \\big(\\mathbf{x}^{(1)},\\dots,\\mathbf{x}^{(55)}\\big)^T=\\begin{bmatrix}\n",
    "    x^{(1)}_{r}  & x^{(1)}_{g}  & x^{(1)}_{b} \\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    x^{(55)}_{r} & x^{(55)}_{g} & x^{(55)}_{b}\n",
    "    \\end{bmatrix} \\in \\mathbb{R}^{N \\times 3}\n",
    "    \\label{xm}\n",
    "    \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "Each image in the training set has a label $y^{(i)}$ where $y^{(i)} = 1$ if the i-th image shows grass while $y^{(i)} = 0$ otherwise (when it shows soil or tiles). It will be convenient to stack the labels into the label vector $\\mathbf{y}$.\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{y}=\\begin{bmatrix}\n",
    "    y^{(1)}\\\\\n",
    "    y^{(2)}\\\\\n",
    "    \\vdots\\\\\n",
    "    y^{(N)}\n",
    "    \\end{bmatrix},\\ i \\in \\{1,\\dots,N \\}\n",
    "    \\label{vy}\n",
    "    \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "### Tasks:\n",
    "- Implement a Python function `get_feature_matrix()` which returns the feature matrix (\\ref{xm}) of size $55 \\times 3$.\n",
    "- Implement a Python function `get_labels()` which returns the label vector (\\ref{vy}).\n",
    "    - The vector should contain the **labels** for each image. The label is `1` if the picture shows grass, `0` if it doesn't.\n",
    "- Implement a Python function `Visualize_data()` to draw three **scatter plots**.\n",
    "    - Make a scatter plot for each color combination (*Greenness vs Redness*,*Greenness vs. Blueness* and *Redness vs Blueness*).\n",
    "        - In these plots, mark a datapoint with a cross if it's a grass picture and with a dot if it's not.\n",
    "    - The plot labels and axes are preconfigured, your job is to setup the correct x and y -values to each `plt.scatter()`-function.\n",
    "    - These scatter plots are a helpful tool for giving us an idea of the relation between the features and the two labels.\n",
    "\n",
    "### Tips:\n",
    "- Since it takes some time to compute the feature matrix $\\mathbf{X}$, you might want to save it as .txt, .csv or any other format that is convenient for you. This way you can just reload the matrix from the file without processing the images again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02c5544d039cf15154c9f6e63784ab9d",
     "grade": false,
     "grade_id": "cell-3d854013ba9d2483",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_feature_matrix(N = 55):\n",
    "    \n",
    "    #initialize the feature vector with zeros. \n",
    "    x_vec = np.zeros((N,3))\n",
    "\n",
    "    x = []\n",
    "    ### STUDENT TASK ###\n",
    "    ## Loop through each picture and each pixel and sum the RGB values into the feature vector matrix.\n",
    "    ## At last, remember to divide each R, G and B sum with the total pixel count to get the average value.\n",
    "    ## Hint: Most of the commands required for this task are in the 2.Dataset-section.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return x_vec;\n",
    "\n",
    "def get_labels(N=55):\n",
    "    y = np.zeros((N,1));\n",
    "    ### STUDENT TASK ###\n",
    "    ## Generate the label vector, where 1 is a Grass image and 0 is Non-Grass.\n",
    "    ## Hint: See the 2.Dataset-section where the picture order is defined.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y\n",
    "\n",
    "\"\"\" VISUALIZE THE DATA \"\"\"\n",
    "def Visualize_data(X,y):\n",
    "\n",
    "    indx_1 = np.where(y == 1) # for grass.\n",
    "    indx_2 = np.where(y == 0) # for non-grass.\n",
    "    \n",
    "    # Set figure size (width, height)\n",
    "    fig, axes = plt.subplots(1, 3,figsize=(15, 5))\n",
    "    # PLOT GREENNESS AGAINST REDNESS\n",
    "    #Make a scatterplot of the average greenness vs redness. \n",
    "    #Indicate Grass images by a cross, and others by a dot.\n",
    "    ### STUDENT TASK ###\n",
    "    #axes[0].scatter(...,..., c='g', marker ='x', label='Grass')\n",
    "    #axes[0].scatter(...,..., c='r', marker ='o', label='Soil+Tiles')\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    axes[0].set_xlabel('Greenness of Images')\n",
    "    axes[0].set_ylabel('Redness of Images')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title(r'$\\bf{Figure\\ 1.}$Green vs Red')\n",
    "\n",
    "    # PLOT GREENNESS AGAINST BLUENESS\n",
    "    #The same as above but now greenness vs blueness.\n",
    "    ### STUDENT TASK ###\n",
    "    #axes[1].scatter(..., ..., c='g', marker ='x', label='Grass')\n",
    "    #axes[1].scatter(..., ..., c='b', marker ='o', label='Soil+Tiles')\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    axes[1].set_xlabel('Greenness of Images')\n",
    "    axes[1].set_ylabel('Blueness of Images')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title(r'$\\bf{Figure\\ 2.}$Green vs Blue')\n",
    "\n",
    "    # PLOT REDNESS AGAINST BLUENESS\n",
    "    #The same as above but now redness vs blueness.\n",
    "    ### STUDENT TASK ###\n",
    "    #axes[2].scatter(..., ..., c='r', marker ='x', label='Grass')\n",
    "    #axes[2].scatter(..., ..., c='b', marker ='o', label='Soil+Tiles')\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    axes[2].set_xlabel('Redness of Images')\n",
    "    axes[2].set_ylabel('Blueness of Images')\n",
    "    axes[2].legend()\n",
    "    axes[2].set_title(r'$\\bf{Figure\\ 3.}$Red vs Blue')\n",
    "    plt.tight_layout()\n",
    "    return axes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b68c86a6b4c4f412ebf828fcb3f96601",
     "grade": true,
     "grade_id": "cell-058cfcc1206a7c5b",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y = get_labels()\n",
    "X = get_feature_matrix()\n",
    "\n",
    "\n",
    "# Full Vector\n",
    "# Let s label : Grass = 1 , Soil = 0, Tiles = 0\n",
    "assert X.shape == (55,3)\n",
    "axes = Visualize_data(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Logistic Regression\n",
    "Our goal is to find out the label $y$ of an image, with $y=1$ if the image shows grass and $y=0$ otherwise, based on its features $\\mathbf{x}$. Similar to linear regression, logistic regression applies a linear function of the form $h^{(\\mathbf{w})}(\\mathbf{x})= \\mathbf{w}^{T} \\mathbf{x}$ to predict the label $y$ based on the features $\\mathbf{x} = \\big(x_{r},x_{g},x_{b} \\big)^{T} \\in \\mathbb{R}^{3}$ of the image. In contrast to linear regression, which uses the squared error loss, here we use the **logistic loss** to measure the quality of a particular classifier $h^{(\\mathbf{w})}$. The logistic loss is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathcal{L}\\big((\\mathbf{x},y),h^{(\\mathbf{w})}\\big) = -y\\log\\big(\\sigma(h^{(\\mathbf{w})}(\\mathbf{x}))\\big)-(1-y)\\log\\big(1-\\sigma(h^{(\\mathbf{w})}(\\mathbf{x}))\\big)\n",
    "    \\label{loss}\n",
    "    \\tag{3}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, which is defined as:\n",
    "\\begin{equation*}\n",
    "    \\sigma(z)= \\frac{1}{1+{\\rm exp}(-z)}.\n",
    "    \\label{sigmoid}\n",
    "    \\tag{4}\n",
    "\\end{equation*}\n",
    "\n",
    "**Note that the expression \\eqref{loss} for the logistic loss applies only if the classes are encoded as $y=1$ and $y=0$. If the two classes are encoded as $y=1$ and $y=-1$, we obtain a different formula for the logistic loss.** \n",
    "\n",
    "Since we have $N=55$ labeled images, each of them characterized by the features $\\mathbf{x}^{(i)}$ and the true label $y^{(i)}$, we can evaluate the logistic loss for all those images to obtain the empirical risk. \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{E}(\\mathbf{w}) & = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}((\\mathbf{x}^{(i)},y^{(i)}),\\ h^{(\\mathbf{w})}) \\nonumber \\\\ \n",
    "&  = \\frac{1}{N} \\sum_{i=1}^{N} -y^{(i)}\\log\\big(\\sigma(\\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big)-(1-y^{(i)})\\log\\big(1-\\sigma(\\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big)\n",
    "   \\label{erm}\n",
    "    \\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "Note that the empirical risk $\\mathcal{E}( \\mathbf{w})$ is a differentiable convex function of the weight vector $\\mathbf{w}$. Therefore, we can use **gradient descent (GD)** to find the weight vector $\\mathbf{w}_{\\rm opt}$ which minimizes the loss function. In particular, GD constructs a sequence of weight vectors $\\mathbf{w}^{(k)}$ by iterating (=repeating) the GD update\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} − \\alpha\\nabla \\mathcal{E}(\\mathbf{w}^{(k)})\n",
    "    \\label{gd}\n",
    "    \\tag{6}\n",
    "\\end{equation*}\n",
    "\n",
    "Assume we run the GD updates for $k$ iterations which results in the weight vector $\\mathbf{w}^{(k)}$ and corresponding classifier map $h^{(\\mathbf{w}^{(k)})}(\\mathbf{x}) = \\big(\\mathbf{w}^{(k)} \\big)^{T} \\mathbf{x}$. Using this classifier map, we can compute the predicted label for a new image with features $\\mathbf{x}=\\big(x_{r},x_{g},x_{b}\\big)^{T}\\in \\mathbb{R}^{3}$ via simple thresholding\n",
    "\n",
    "\\begin{equation*} \n",
    "    \\hat{y} = \\begin{cases} \n",
    "        1 &\\text{if}\\ \\sigma\\big(h^{(\\mathbf{w}^{(k)})}(\\mathbf{x})\\big) \\geq 1/2\\\\\n",
    "        0 &\\text{if}\\ \\sigma\\big(h^{(\\mathbf{w}^{(k)})}(\\mathbf{x})\\big) < 1/2\n",
    "    \\end{cases}\n",
    "    \\label{eq_classify}\n",
    "    \\tag{7}\n",
    "\\end{equation*}\n",
    "\n",
    "### Tasks\n",
    "- Implement a Python function `sigmoid_function(z)` which returns the function value (\\ref{sigmoid}) of the sigmoid function.\n",
    "    - In our case $z$ is the dot product of the weight vector and the feature vector. \n",
    "- Implement a Python function `gradient(X,y,w)` which takes as input the feature matrix $\\mathbf{X}$, the label vector $\\mathbf{y}$ and the weight vector $\\mathbf{w}$. The function should return the gradient of the empirical risk $\\mathcal{E}(\\mathbf{w})$ (\\ref{erm}) at the vector $\\mathbf{w}$.\n",
    "- Implement a Python function `logisticRegression_func(X,y,step_size, K)` which takes as input the feature matrix $\\mathbf{X}$, the label vector $\\mathbf{y}$, the GD step size `step_size` and the number `K` of GD steps. The function should deliver the final GD iterate $\\mathbf{w}^{(K)}$ and a vector of length K whose $k^{th}$ entry is the empirical risk $\\mathcal{E}(\\mathbf{w}^{(k)})$.\n",
    "- Implement a Python function `predict_output()` which takes as input the feature matrix $\\mathbf{X}$ and the optimal weight vector $\\mathbf{w}$ and returns a vector of length $N$ containing the predicted labels $\\mathbf{\\hat{y}}$ (see Eq. (\\ref{eq_classify})). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16178fdbb893f7335e84c9fec29bcb11",
     "grade": false,
     "grade_id": "cell-42a036d9b0cba489",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_func(z):\n",
    "    ### STUDENT TASK ###\n",
    "    # sigmoid = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return sigmoid\n",
    "\n",
    "def gradient(X,y,w):\n",
    "    ### STUDENT TASK ###\n",
    "    # grad = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return grad\n",
    "\n",
    "def logisticRegression_func(X,y,step_size, K):\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    # Initialize w as 1xd array.\n",
    "    w = np.zeros((1,d))\n",
    "    loss = float('inf')\n",
    "    loss_list = []\n",
    "    for i in range(K):\n",
    "        ### STUDENT TASK ###\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return loss_list, w\n",
    "\n",
    "\"\"\" Predict Output \"\"\"\n",
    "def predict_output(X,w):\n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2559ffa71f36871dfb5c658689fc39e",
     "grade": true,
     "grade_id": "cell-80bab771073c9131",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell. Do not modify.\n",
    "step_size = 1e-5\n",
    "num_iter = 3000\n",
    "e_list, w_opt = logisticRegression_func(X,y,step_size,num_iter)\n",
    "print ('The optimal weight vector is:', w_opt)\n",
    "y_hat = predict_output(X,w_opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32cddfec983ce0dc9f0b4cd30b59c6eb",
     "grade": false,
     "grade_id": "cell-6570b18f56ded61e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3.3 Gradient Descent Step Size\n",
    "The performance of GD depends crucially on the step size $\\alpha$. This task requires you to investigate how different choices for the step size influence the behavior of GD. \n",
    "\n",
    "### Tasks\n",
    "- Implement a Python function `visualize_error()` which iterates over different step sizes given in array `step_sizes=[0.1,0.5,1,5,10,16]`.\n",
    "    - Use `logisticRegression_func()` which you implemented in task`3.2: Logistic Regression` with each step size, it should return a list of errors.\n",
    "    - Plot each of these error lists. You should see two plots with 6 descending lines.\n",
    "    - Figure out which step size has the fastest convergence and mark the corresponding curve in red (i.e. by changing `best=None` to right step size. For example if you think that the best step size is 1, choose `best=1`). Now you should see a plot where one line is red and rest of them are blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "904146c8a573cb0feba0468241169a6a",
     "grade": false,
     "grade_id": "cell-e8deda7ea56f6261",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_error(X, y, step_sizes, best = None, num_iter = 2000):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    fig, axes = plt.subplots(1, 2,figsize=(12, 4))\n",
    "    for step in step_sizes:\n",
    "        ### STUDENT TASK ###\n",
    "        # Plot Error against Step Size\n",
    "        # loss_list, w_opt = \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        n = len(loss_list) # Size of list remains the same.\n",
    "        x_axes = np.linspace(0,n,n,endpoint=False)\n",
    "        axes[0].plot(x_axes, loss_list, label=step)\n",
    "    axes[0].set_xlabel('Number of Iterations')\n",
    "    axes[0].set_ylabel('Loss Function')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title(r'$\\bf{Figure\\ 4.}$Converge of GD')\n",
    "    \n",
    "    for step in step_sizes:\n",
    "        ### STUDENT TASK ###\n",
    "        # Plot Error against Step Size.\n",
    "        # Now mark the best converge in red. Use value from best as a correct step size.\n",
    "        # loss_list, w_opt = \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        n = len(loss_list) # Size of list remains the same.\n",
    "        x_axes = np.linspace(0,n,n,endpoint=False)\n",
    "        if step == best:\n",
    "            axes[1].plot(x_axes, loss_list, label=step, color=\"red\")\n",
    "        else:\n",
    "            axes[1].plot(x_axes, loss_list, label=step, color=\"blue\")\n",
    "    \n",
    "    axes[1].set_xlabel('Number of Iterations')\n",
    "    axes[1].set_ylabel('Loss Function')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title(r'$\\bf{Figure\\ 5.}$Converge of GD')\n",
    "    plt.tight_layout()\n",
    "    return best, axes\n",
    "\n",
    "### STUDENT TASK ###\n",
    "# Change best=None into step size from the list that provides the fastest converge. e.g best=1\n",
    "res0_1, axes = visualize_error(X/255, y, best=None, step_sizes=[0.1,0.5,1,5,10,16])\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aedf66b036e1a227b5e8abdbb7049382",
     "grade": true,
     "grade_id": "cell-76b0b20fe659e6c3",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5082605ac1be0672b3534bc8449c7cd",
     "grade": false,
     "grade_id": "cell-16b2b1e46281f69a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3.4 Accuracy - How well did we do?\n",
    "In order to assess how well our model works, we calculate the accuracy achieved by the classifier $h^{(\\mathbf{w})}$ obtained from task 3.3. We do this by computing the fraction of correctly labeled images, i.e., for which the true label $y^{(i)}$ is equal to the predicted label $\\hat{y}^{(i)}$:\n",
    "\\begin{equation*}\n",
    "    \\text{Accuracy} =\\dfrac{1}{N} \\sum_{i=1}^{N} \\mathcal{I}(\\hat{y}^{(i)} = y^{(i)})\n",
    "    \\label{acc}\n",
    "    \\tag{8}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "### Tasks\n",
    "- Implement a Python function `calculate_accuracy(y,y_hat)` according to (Eq. \\ref{acc}) which takes as inputs $\\mathbf{y}$ and $\\mathbf{\\hat{y}}$ and returns the accuracy (as percentage). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee87d324aece022e4fbac7dbecd12140",
     "grade": false,
     "grade_id": "cell-46b5d6918ae35a38",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y,y_hat):\n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return accuracy\n",
    "print ('Accuracy of the result is: %f%%' % calculate_accuracy(y,y_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c9c8bafbe25cd5a57ba5ed511385667",
     "grade": true,
     "grade_id": "cell-62e401362b014895",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1538d7f9592262df197ef53e5445d437",
     "grade": false,
     "grade_id": "cell-130eebe8963b8793",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3.5: Multiclass Classification (\"One vs All\" or \"One vs Rest\")\n",
    "We have succesfully implemented a logistic regression model which classifies the current on-board image as showing grass or not. We will now extend this model to know how to classify the image according to the three categories \"grass\", \"soil\" or \"tiles\". \n",
    "\n",
    "So is all of our previous work on classifying images into \"grass\" vs. \"no grass\" for nothing? Nope! Adapting a binary classification method (using two different label values) to this multiclass task is straightforward using the **“one vs all” technique**. The idea is quite simple: split the multiclass problem into three subproblems, each subproblem being a binary classification problem as considered in task `3.3 Logistic Regression`. In particular, we can solve the problem of classifiying images into classes \"grass\", \"soil\" or \"tiles\" by solving the three subproblems:\n",
    "* subproblem 1: classify images into \"grass\"(y=1) vs. \"no grass\"(y=0) \n",
    "* subproblem 2: classify images into \"soil\"(y=1) vs. \"no soil\"(y=0) \n",
    "* subproblem 3: classify images into \"tiles\"(y=1) vs. \"no tiles\"(y=0) \n",
    "\n",
    "We can apply the method developed under `3.2: Logistic Regression`, to solve each of these subproblems. The first subproblem has already been solved in Sec. 3.3, so we only need to solve the latter two subproblems. Note that for the three subproblems we use the same feature vector $\\mathbf{x}^{(i)}=\\big(x_{r}^{(i)},x_{g}^{(i)},x_{b}^{(i)}\\big)^{T}$ but a different label $y^{(i)}$ for the $i$th image. In particular, the first image (which shows grass) has label $y^{(1)}=1$ in subproblem 1 but $y^{(1)}=0$ in subproblem 2 and subproblem 3.\n",
    "\n",
    "For each subproblem we compute the optimal weight vector $\\mathbf{w}^{(\\rm grass)}$,$\\mathbf{w}^{(\\rm soil)}$, $\\mathbf{w}^{(\\rm tiles)}$ by solving the empirical risk minimization problem (5). \n",
    "\n",
    "Assume we want to classify a new image with features $\\mathbf{x}=(x_{r},x_{g},x_{b})^{T}$ yielding the following predictor values : \n",
    "- $h^{(\\mathbf{w}^{(\\rm grass)})}(\\mathbf{x}) = 2$ (\"grass vs. no grass\")\n",
    "- $h^{(\\mathbf{w}^{(\\rm soil)})}(\\mathbf{x}) = 1$ (\"soil vs. no soil\") \n",
    "- $h^{(\\mathbf{w}^{(\\rm tiles)})}(\\mathbf{x}) = 5$ (\"tiles vs. no tiles\")\n",
    "\n",
    "Thus, the predictor $h^{(\\mathbf{w}^{(\\rm tiles)})}(x)$ for subproblem 3 (`tiles` vs. `no tiles`) yields the highest confidence. Hence, we classify this image as `tiles`. \n",
    "\n",
    "![](./images/MulticlassHunda.jpg)\n",
    "\n",
    "### Tasks\n",
    "- Implement a Python function `get_labels_k()` which takes as input the number of data points $N$ and the index $k$ of the subproblem: $k=0$ means subproblem 1, $k=1$ means subproblem 2 and $k=2$ means subproblem 3, and returns the label vector $\\mathbf{y}$ for that category. \n",
    "    - i.e. either Grass, Soil **or** Tile pictures are `1`, the other 2 are `0`\n",
    "- Implement a Python function `multiclass()` which implements logistic regression for the three subproblems and then outputs the real labels $\\mathbf{y}$, the final predicted labels $\\mathbf{\\hat{y}}$ and the accuracy. \n",
    "    - Use the `logistic_regression()` and `sigmoid_func()` of section 3.3 to get the optimal weight vector and the probability of the picture belonging to that class respectively. \n",
    "    - After you have done logistic regression for each of the three categories, predict the class the picture belongs to by choosing the highest probability of all binary classifiers.\n",
    "    - Use the function `calculate_accuracy` of section 3.4 to calculate the accuracy. \n",
    "    - You might want to encode the three classes \"Grass\", \"Soil\" and \"Tiles\" using numbers, e.g. \"Grass\"=0, \"Soil\"=1 and \"Tiles\"=2\n",
    "    - **Note**: the label vector $\\mathbf{y}$ of this function is different than the output from `get_labels_k()`\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60400ed11dfa4106f3db0a1ef6e79b71",
     "grade": false,
     "grade_id": "cell-3fa563fa2e03f78c",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_labels_k(N=55,k=0):\n",
    "    y = np.zeros((N,1));\n",
    "    ### STUDENT TASK ###\n",
    "    ## Generate the label vector which has value 1 for the pictures of the category we are currently looking at (indicated by k) \n",
    "    ## and 0 for the other two categories. \n",
    "    ## Hints: See the 2. Dataset-section where the picture order is defined\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y\n",
    "\n",
    "def multiclass():\n",
    "    y_predict = []\n",
    "    step_size = 1e-5\n",
    "    num_iter = 3000\n",
    "    for i in range(0,3):\n",
    "        ### STUDENT TASK ###\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return y, y_hat, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1a776b21399439986fae8cee9ef9475",
     "grade": true,
     "grade_id": "cell-3dbcccaca59cbc70",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y, y_hat,acc = multiclass()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
