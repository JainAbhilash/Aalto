{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68ba681e4b607e1ef1c41de2b5476001",
     "grade": false,
     "grade_id": "cell-0ef3a793cfc0e37b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Machine Learning: Basic Principles 2018\n",
    "# Round 2 - Regression\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "In this exercise you will learn how to use **linear regression**, in order to predict a quantity of interest based on data. The implementation of linear regression amounts to the minimization of a function (**the empirical risk**) which measures the error of a predictor when applied to training data. You will hear about **gradient descent (GD)** which is a simple but powerful algorithm for finding the minimum of a function. Variations of GD are the main algorithmic tools behind many state-of-the art machine learning methods (such as deep learning). \n",
    "A good understanding of GD is therefore a worthy asset for an ML engineer. Expanding the basic linear regression model you will learn some additional techniques such as **feature scaling** (or **data normalization**) \n",
    "and **feature mapping** to obtain **polynomial regression**. \n",
    "\n",
    "(Hint: Completing this exercise will help you in future rounds where you might \n",
    "reuse substantial parts of the code developed in this exercise.)\n",
    "\n",
    "    \n",
    "### Keywords\n",
    "\n",
    "`Linear Regression`, `Gradient Descent (GD)`, `Mean Squared Error (MSE)`, `Empirical Risk Minimization (ERM)`, `Polynomial Regression`,`Data Visualization`\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "In this exercise we will apply __linear regression__ to explore the dependencies between the two cryptocurrencies **Ethereum** and **Bitcoin**. In particular, we will try to __predict the price of the cryptocurrency Ethereum [2] from the price of the cryptocurrency Bitcoin [1]__.\n",
    "\n",
    "> ### References\n",
    ">(only background info, not required for completing the exercise)\n",
    ">\n",
    ">[1] Cointelegraph, 2013-2018. What is bitcoin? Here's everything you need to know. Blockchains, bubbles and the future of money. https://www.cnet.com/how-to/what-is-bitcoin/ [Accessed 13th May 2018].\n",
    ">\n",
    ">[2] Jaffe, J., 2018. What is Ethereum. Guide for Beginners. https://cointelegraph.com/ethereum-for-beginners/what-is-ethereum  [Accessed 13th May 2018].\n",
    ">\n",
    "\n",
    "\n",
    "## 2 The Data\n",
    "In order to predict one currency from the other we start by looking at some historic data. The data is stored in two files; one file for each cryptocurrency:\n",
    "* BTC-USD.csv (Bitcoin prices in US Dollar)\n",
    "* ETC-USD.csv (Ethereum prices in US dollar)\n",
    "\n",
    "These files contain time series of daily prices (in US dollar) and traded volumes of Bitcoin and \n",
    "Etherum throughout the last three years. The files can be found on the website https://finance.yahoo.com\n",
    "\n",
    "The following code snippet provides you some basic tools for reading in the dataset stored in these files and how to visualize this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d88f8556104f72084f5d850408972c73",
     "grade": true,
     "grade_id": "cell-b41dbb88d89db329",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # import Pandas library (and defining shorthand \"pd\") for reading and manipulating the data files\n",
    "from matplotlib import pyplot as plt # import and define shorthand \"plt\" for library \"pyplot\" providing plotting functions\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np   # import and define shorthand \"np\" for library \"numpy\" for advanced mathematical operations in python\n",
    "\n",
    "#read in data from csv files \n",
    "#parse_dates function is used on Date-column to change them from string to date-object\n",
    "\n",
    "df_bc = pd.read_csv(\"BTC-USD.csv\",parse_dates=['Date']) \n",
    "df_eth = pd.read_csv(\"ETH-USD.csv\",parse_dates=['Date']) \n",
    "\n",
    "\n",
    "## Show top rows of each file.\n",
    "# function \"display()\" is Jupyter Notebook command to show multiple function outputs from one cell. \n",
    "display(HTML(df_bc.head(5).to_html(max_rows=5)))\n",
    "display(HTML(df_eth.head(5).to_html(max_rows=5)))\n",
    "\n",
    "## Plot original data to same figure\n",
    "# Example of use: plt.plot(x,y)\n",
    "plt.plot(df_bc.Date.values, df_bc.Close.values, label=(\"Bitcoin price (in USD)\")) \n",
    "plt.plot(df_eth.Date.values, df_eth.Close.values, label=(\"Ethereum price (in USD)\"))\n",
    "\n",
    "## set up figure title and labels for x- and y-axis\n",
    "plt.title(r'$\\bf{Figure\\ 1.}$ Bitcoin vs Ethereum')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('price')\n",
    "\n",
    "## rotate x-ticks by 20 degrees\n",
    "plt.xticks(rotation=20)\n",
    "\n",
    "## enable legend-box for plot labels\n",
    "plt.legend()\n",
    "\n",
    "## show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c024ddd4e8f098933c9504634c187181",
     "grade": false,
     "grade_id": "cell-007ebcbadc6ef155",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3 Exercise\n",
    "\n",
    "Our ML problem involves data points $\\mathbf{z}$ which contain all parameters of the two cryptocurrencies Bitcoin and Ethereum at a particular day. As the feature $x$ of a data point $\\mathbf{z}$ we choose the closing price of Bitcoin on a particular day, while the label $y$ is the Ethereum closing price.\n",
    "\n",
    "The actual exercise starts from here and it has been divided into 6 tasks:\n",
    "\n",
    "* 3.1 Getting Hands on the Data\n",
    "* 3.2 Scatterplots\n",
    "* 3.3 Linear Regression\n",
    "* 3.4 Polynomial Regression\n",
    "* 3.5 Gradient Descent\n",
    "\n",
    "Your task is to fill in `...` under `### STUDENT TASK ###` in each step.\n",
    "\n",
    "## 3.1 Getting Hands on the Data\n",
    "\n",
    "In order to learn (or find) a good predictor $h(x)$ which approximates the Ethereum closing price $y$ based on the Bitcoin closing price $x$, we make use of the data stored in the two csv files, `BTC-USD.csv` and `ETC-USD.csv`. In particular, these files provide us with a labeled dataset $\\mathcal{X} = \\{(x^{(i)},y^{(i)} \\}_{i=1}^{N}$. The dataset $\\mathcal{X}$ contains $N$ data points $z^{(i)}$, for $i=1,\\ldots,N$, with the features $x^{(i)}$ which we define as the closing price of Bitcoin at day $i$. The quantity of interest (the label) $y^{(i)}$ of the $i$-th data point is the closing price of Ethereum at day $i$. We would like to predict the Ethereum closing price $y^{(i)}$ solely from the Bitcoin closing price $x^{(i)}$ on the same day.\n",
    "\n",
    "\n",
    "As evident from Figure 1, comparing the closing prices of Bitcoin and Ethereum is difficult visually because of the significantly different value ranges of Bitcoin and Ethereum prices. Therefore, in order to facilitate a better intuition for the relation between the two currencies, we will normalize the prices to the same value range [3]:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{scale_features_labels}\n",
    "\\tag{1}\n",
    " x^{(i)}_{\\rm scaled} = \\frac{x^{(i)}-\\min_{i} x^{(i)}}{\\max_{i} x^{(i)}-\\min_{i} x^{(i)}}, \n",
    "   y^{(i)}_{\\rm scaled} = \\frac{y^{(i)}-\\min_{i} y^{(i)}}{\\max_{i} y^{(i)}-\\min_{i} y^{(i)}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The normalization results in having the same value range [0,1] for both currencies.\n",
    "\n",
    "> ### References\n",
    "> \n",
    "> [3] Feature Scaling. In Wikipedia. https://en.wikipedia.org/wiki/Feature_scaling#Methods [Accessed 13th June 2018].\n",
    ">\n",
    "\n",
    "### Tasks:\n",
    "1. Read in the closing prices of Ethereum and Bitcoin (stored in column with the caption \"Close\") from the csv files\n",
    "2. Preprocess the data\n",
    "    1. Normalize \\eqref{scale_features_labels} Bitcoin prices from column called __Close__ and save them into a variable `bitcoin`.\n",
    "    2. Normalize \\eqref{scale_features_labels} Ethereum prices from column called __Close__ and save them into a variable `ethereum`.\n",
    "4. Create a **Figure 2** which depicts the time series of rescaled prices. \n",
    "\n",
    "The following code snippet imports all Python libraries which are required for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa79c8b2ba233e1df7aed180a52a76ae",
     "grade": false,
     "grade_id": "cell-7522fed1708c99c2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# import all libraries to be used in this exercise\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97f4d274c34a4074b0513f7ef809cbcb",
     "grade": false,
     "grade_id": "cell-ebf6b468dca0de25",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def solution1():\n",
    "    # We assign Global variables for testing purposes\n",
    "    global df_bc, df_eth, bitcoin, ethereum, axis, best_alpha\n",
    "    # read in historic Bitcoin and Ethereum statistics data from the files \"BTC-USD.csv\" and \"ETH-USD.csv\"\n",
    "\n",
    "    df_bc = pd.read_csv(\"BTC-USD.csv\", parse_dates=['Date'])\n",
    "    df_eth = pd.read_csv(\"ETH-USD.csv\", parse_dates=['Date'])\n",
    "\n",
    "    # read the dates for the individual records from column \"Date\"\n",
    "    bitcoin_date = df_bc.Date.values\n",
    "    ethereum_date = df_eth.Date.values\n",
    "\n",
    "    ### STUDENT TASK ###\n",
    "    ## read in closing prices for Bitcoin and Ethereum from the column \"Close\" and computed the rescaled values  \n",
    "    ## Replace '...' with your solution.\n",
    "    #bitcoin = [...]\n",
    "    #ethereum = [...]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Show cryptocurrency prices over transaction date\n",
    "    axis = plt.gca()\n",
    "    plt.plot(bitcoin_date,bitcoin, label=(\"Bitcoin\")) \n",
    "    plt.plot(ethereum_date,ethereum, label=(\"Ethereum\"))\n",
    "    plt.title(r'$\\bf{Figure\\ 2.}$ Normalized cryptocurrency prices')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized price')\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "177328912267bf40dbfd9ca24a590bae",
     "grade": true,
     "grade_id": "cell-b2533633b93586ec",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "solution1()\n",
    "assert len(bitcoin) == 948,\"Your bitcoin data is incorrect length\"\n",
    "assert len(ethereum) == 948, \"Your ethereum data is incorrect length\"\n",
    "assert np.max(bitcoin)== np.max(bitcoin), \"Incorrect max values after normalisation\"\n",
    "assert np.min(bitcoin)== np.min(bitcoin), \"Incorrect min values after normalisation\"\n",
    "\n",
    "# Run test that check that plot renders correctly. Requires plotchecker to be installed.\n",
    "from plotchecker import LinePlotChecker\n",
    "pc = LinePlotChecker(axis)\n",
    "pc.assert_num_lines(2)\n",
    "pc.find_permutation('title',r'$\\bf{Figure\\ 2.}$ Normalized cryptocurrency prices')\n",
    "pc.find_permutation('xlabel','Date')\n",
    "pc.find_permutation('ylabel','Normalized price')\n",
    "pc.assert_labels_equal(['Bitcoin','Ethereum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b03868e327440c4ba5549a03b9aac47",
     "grade": false,
     "grade_id": "cell-b85dc13f39e40ddd",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3.2 Scatterplots\n",
    "\n",
    "Scatter plots are a helpful tool for getting an idea of the relation between features and labels. \n",
    " \n",
    "### Task:\n",
    "1. Generate a scatter plot using the Python function `plt.scatter()`. The x-axis of the scatter plot represents the values of the rescaled Bitcoin prices $x_{\\rm scaled}^{(i)}$ and y-axis represents the values of the rescaled Ethereum prices $y_{\\rm scaled}^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae6f29d32c601fef56d30fc6f8f3d23d",
     "grade": false,
     "grade_id": "cell-9e4c40988deb13df",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "#x=...\n",
    "#y=...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x,y)\n",
    "plt.title(r'$\\bf{Figure\\ 3.}$Normalized cryptocurrency prices ($x^{(i)},y^{(i)}$)')\n",
    "plt.xlabel('normalized Bitcoin prices')\n",
    "plt.ylabel('normalized Ethereum prices')\n",
    "plt.annotate('$(x^{(i)},y^{(i)})$', xy=(x[913], y[913]), xytext=(0.1, 0.5),\n",
    "            arrowprops=dict(arrowstyle=\"->\",facecolor='black'),\n",
    "            )\n",
    "axis = plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "347f9bce0724f81e1598a45ee40a6b12",
     "grade": true,
     "grade_id": "cell-e2b94b870e5dc954",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Check that plot is a scatterplot. Requires plotchecker\n",
    "from plotchecker import ScatterPlotChecker\n",
    "pc = ScatterPlotChecker(axis)\n",
    "assert len(pc.x_data) == 948\n",
    "assert len(pc.y_data) == 948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ced30d52f79bbc151fb45cebd7544c1",
     "grade": false,
     "grade_id": "cell-ba1d8078dc67137d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3.3 Linear Regression \n",
    "\n",
    "Our goal is to predict the closing price $y^{(i)}$ of Ethereum at some day $i$ based on the Bitcoin closing price $x^{(i)}$ on the very same day. Within linear regression, we try to do this prediction using linear functions which belong to the hypothesis space\n",
    "\n",
    "\\begin{equation*}\n",
    " \\mathcal{H} = \\{h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} \\mbox{ for some } \\mathbf{w} \\in \\mathbb{R}\\}.\n",
    "\\label{eq1}\n",
    "\\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "In particular, we try to predict (or approximate) $y^{(i)}$ by the function value $h^{(\\mathbf{w})}(\\mathbf{x}^{(i)}) = \\mathbf{w}^{T}\\mathbf{x}^{(i)}$ using the augmented feature vector $\\mathbf{x}^{(i)} = (x^{(i)},1)^{T}$. At first sight, it seems strange to add a constant one to each feature value $x^{(i)}$ (which is the closing price of Bitcoin at day i). However, it will turn out to allow for a larger class of functions $h^{(\\mathbf{w})}$ which can be used for the prediction.\n",
    "\n",
    "The prediction $h^{(\\mathbf{w})}(\\mathbf{x}^{(i)})$ will typically incur a non-zero __prediction error__ $y^{(i)} - h^{(\\mathbf{w})}(\\mathbf{x}^{(i)})$, which we quantify using its square $(y^{(i)} - h^{(\\mathbf{w})}(\\mathbf{x}^{(i)})^2$ (squared error loss). Since we have $N$ data points $(x^{(i)},y^{(i)})$ available, we can compute the __empirical risk__ (average loss)\n",
    "\\begin{equation*}\n",
    " \\mathcal{E} (\\mathbf{w}) = \\frac{1}{N}\\sum^{N}_{i=1}(y^{(i)} - \\mathbf{w}^{T} \\mathbf{x}^{(i)})^2.\n",
    "\\label{eq2}\n",
    "\\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Our goal is to find the optimal predictor $h_{\\rm opt}(\\cdot)$: \n",
    "\\begin{equation}\n",
    "h_{\\rm opt}(\\cdot) = \\underset{{h(\\mathbf{w})} \\in \\mathcal{H}}{\\operatorname{argmin}} \\mathcal{E} (\\mathbf{w}).\n",
    "\\label{eq3}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Note that every function $h^{(\\mathbf{w})}(\\mathbf{x}) \\in \\mathcal{H}$ corresponds to a particular choice of the weight vector $\\mathbf{w} \\in \\mathbb{R}^{2}$. This allows us to rewrite the optimization problem (Eq. \\ref{eq3}) as an optimization problem for the weight vector: \n",
    "\\begin{align}\n",
    "\\mathbf{w}_{\\rm opt} &= \\underset{ \\mathbf{w} \\in \\ \\mathbb{R}^{2}}{\\operatorname{argmin}} \\{ \\frac{1}{N}\\sum^{N}_{i=1}(y^{(i)} - \\mathbf{w}^{T} \\mathbf{x}^{(i)})^2 \\}\\\\\n",
    "&= \\underset{ \\mathbf{w} \\in \\ \\mathbb{R}^{2}}{\\operatorname{argmin}} \\frac{1}{N} {||\\mathbf{y}-\\mathbf{X}\\mathbf{w}||}_2^2,\n",
    "\\label{eq4}\n",
    "\\tag{4}\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\mathbf{X} = \\big(\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(N)}\\big)^{T} \\text{and }\\mathbf{y} = \\begin{bmatrix}y^{(1)}\\\\\\vdots\\\\y^{(n)}\\end{bmatrix}\\ \\mathbf{X} \\in \\mathbb{R}^{N \\times 2},\\mathbf{y}\\in\\mathbb{R}^{N}.\n",
    "\\end{align}\n",
    "An optimal weight vector $\\mathbf{w}_{\\rm opt}$ solving \\eqref{eq4} induces an optimal predictor $h_{\\rm opt}= h^{(\\mathbf{w}_{\\rm opt})}$ which solves \\eqref{eq3}. \n",
    "\n",
    "Whenever the matrix $\\mathbf{X}^T \\mathbf{X}$ is invertible, the solution of (\\ref{eq4}), i.e., the optimal weight which minimizes the empirical risk, is obtained as \n",
    "\\begin{align}\n",
    "\\mathbf{w}_{\\rm opt} = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}.\n",
    "\\label{eq5}\n",
    "\\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### Tasks\n",
    "0. Implement a Python function `featureMatrix(x)` which takes a length-$N$ vector of Bitcoin closing prices $x^{(i)}$, for $i=1,\\ldots,N$ as input and outputs the feature matrix $\\mathbf{X}=\\big(\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(N)}\\big)^{T} \\in \\mathbb{R}^{N \\times 2}$ whose rows are the augmented feature vectors $\\mathbf{x}^{(i)} = \\big(x^{(i)},1\\big)^{T} \\in \\mathbb{R}^{2}$. \n",
    "1. Implement a Python function `fit(X,y)` that takes feature matrix $\\mathbf{X}$ and label vector $\\mathbf{y}$ as inputs and returns the optimal weight vector according to (Eq. \\ref{eq5}.) \n",
    "2. Implement a Python function `predict(X,w)` that takes the feature matrix $\\mathbf{X}$ and weight vector $\\mathbf{w}$ as input and returns a vector containing the predicted labels according to (Eq. \\ref{eq1}.)\n",
    "3. Implement a Python function `empirical_risk(X, y, w)` taking feature matrix $\\mathbf{X}$, label vector $\\mathbf{y}$ and weight vector $\\mathbf{w}$ and returns the empirical error (Eq. \\ref{eq2}).\n",
    "4. Implement a Python function `linearRegression(X,y)` which outputs the empirical error of your prediction and the optimal weight vector\n",
    "    * Modify our feature matrix and label vector by adding a dummy feature (i.e. 1). This can be done by using helper functions `featureMatrix(x)` and `labelVector(y)`\n",
    "    * use these modified $\\mathbf{X}$ and $\\mathbf{y}$ to fit the data and calculate the empirical error using the Python functions that you defined in previous steps.\n",
    "5. Execute the cell. You should see a plot similar to $\\bf{Figure\\ 3.}$, but with a legend showing the empirical error and your prediction line in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2c10bd38c5f4a1f6c93d631d5cc8e28",
     "grade": false,
     "grade_id": "cell-b42f92f41dd1972c",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def featureMatrix(x):\n",
    "    # Generate x_i = (x_i, 1) feature matrix\n",
    "    # x_i = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return x_i\n",
    "\n",
    "def fit(x, y):\n",
    "    ### STUDENT TASK ### \n",
    "    ## Compute optimal w by replacing '...' with your solution.\n",
    "    ## Hints: Check out numpy's linalg.inv(), dot() and transpose() functions.\n",
    "    # w_opt = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return w_opt\n",
    "\n",
    "# Predict new y data\n",
    "# return numpy.ndarray:\n",
    "#      [ [y_pred_1]\n",
    "#          ....\n",
    "#        [y_pred_N]]\n",
    "def predict(X, w_opt):\n",
    "    ### STUDENT TASK ###\n",
    "    ## Predict new y data by replacing '...' with your solution.\n",
    "    ## Hint! Use X and w_opt to get necessary matrices.\n",
    "    # y_pred ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y_pred\n",
    "\n",
    "# Calculate empirical error of the prediction\n",
    "# return float\n",
    "def empirical_risk(X, Y, w_opt):\n",
    "    ### STUDENT TASK ###\n",
    "    ## Compute empirical error by replacing '...' with your solution.\n",
    "    ## Hints! Use X, Y and w_opt to get necessary matrices.\n",
    "    ##        Check out numpy's dot(), mean(), power() and subtract() functions.\n",
    "    # empirical_error = ... \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return empirical_error\n",
    "    \n",
    "def linearRegression(x, y):\n",
    "    ### STUDENT TASK ###\n",
    "    ## Calculate X, Y, w_opt and empirical_error\n",
    "    ## Hints! Use featureMatrix() and labelVector() to get necessary matrices, X and Y.\n",
    "    # X = ... \n",
    "    # Y = ...\n",
    "    # w_opt=...\n",
    "    # empirical_error=...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return w_opt, empirical_error\n",
    "\n",
    "def labelVector(y):\n",
    "    # Reshape y to ensure correct behavior when doing matrix operations\n",
    "    return np.reshape(y,(len(y),1))\n",
    "\n",
    "def draw_plot(x, y, title=''):\n",
    "    w_opt, empirical_error = linearRegression(x, y)\n",
    "    x_pred = np.linspace(0,1.1,100)\n",
    "    y_pred = predict(featureMatrix(x_pred), w_opt)\n",
    "    # Plot data points and linear regression fitting line\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x_pred,y_pred,'r', label=(\"Empirical = %.4f\" % empirical_error))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Bitcoin')\n",
    "    plt.ylabel('Ethereum')\n",
    "    plt.legend()\n",
    "    axis = plt.gca()\n",
    "\n",
    "    \n",
    "######### Linear regression model for x and y data #########\n",
    "draw_plot(x, y, r'$\\bf{Figure\\ 4.}$ Normalized cryptocurrency prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34b25cfbab6d323eaae2a2ca26f42928",
     "grade": true,
     "grade_id": "cell-350667178cd8af68",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "w_opt, empirical_error = linearRegression(x, y)\n",
    "assert empirical_error < 0.015\n",
    "w_opt, empirical_error = linearRegression([0,1,2,3], [0,1,2,3])\n",
    "# Because of computational rounding errors, empirical error is almost never exactly 0\n",
    "assert empirical_error < 1e-30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84db54655a3aaf7638d3e8c9438375ab",
     "grade": false,
     "grade_id": "cell-e2d92222a1a0d18c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3.4 Polynomial Regression\n",
    "\n",
    "By looking at $\\bf{Figure\\ 3.}$ we see that relation between label $y$ (Ethereum price) and feature $x$ (Bitcoin price) is highly non-linear. Therefore it is useful to consider a hypothesis space which is constituted by polynomial functions\n",
    "\\begin{equation}\n",
    "\\label{equ_def_poly_hyposapce}\n",
    "\\mathcal{H}^{(d)}_{\\rm poly} = \\{ h^{(\\mathbf{w})}(\\cdot): \\mathbb{R} \\rightarrow \\mathbb{R}: h^{(\\mathbf{w})}(x) = \\sum_{r=1}^{d} w_{r} x^{r-1} \\mbox{, with some } \\mathbf{w} =(w_{1},\\ldots,w_{d})^{T} \\in \\mathbb{R}^{d} \\}.\n",
    "\\end{equation}  \n",
    "\n",
    "As in task `3.3 Linear Regression`, the quality of a predictor is measured by the squared error loss and linear regression amounts to minimizing the  average squared error loss (mean squared error):\n",
    "\\begin{equation} \n",
    "\\min_{h \\in \\mathcal{H}_{\\rm poly} } \\frac{1}{N} \\sum_{i=1}^{d} (y^{(i)} - h^{(\\mathbf{w})}(x^{(i)}))^{2} \\mbox{, where } h^{(\\mathbf{w})}(x)=\\sum_{r=1}^{d} w_{r} x^{r-1}.\n",
    "\\end{equation} \n",
    "\n",
    "As discussed in the course book (Section 3.2 \"Polynomial Regression\") polynomial regression is equivalent to combining linear regression with a feature map. In particular, we transform the features $x$ (Bitcoin closing price) of the data points to a higher dimensional feature space using the feature map \n",
    "\n",
    "\\begin{equation}\n",
    "\\phi(x) = (x^{d-1},..,x^1, x^0)^{T} \\in \\mathbb{R}^{d}.\n",
    "\\label{eq7}\n",
    "\\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "This feature map takes the original feature $x \\in \\mathbb{R}$ (the Bitcoin closing price) as input and returns a new feature vector $\\mathbf{x}= \\phi(x) \\in \\mathbb{R}^{d}$ of length $d$. The resulting feature matrix is  \n",
    "\\begin{equation*}\n",
    "    \\mathbf{X}_{poly}=\\begin{bmatrix}\n",
    "        (x^{(1)})^{d-1} & \\dots & (x^{(1)})^{1}& (x^{(1)})^{0}\\\\\n",
    "        (x^{(2)})^{d-1} & \\dots & (x^{(2)})^{1}& (x^{(2)})^{0}\\\\\n",
    "      \\vdots & \\ddots& \\vdots & \\vdots\\\\\n",
    "       (x^{(N-1)})^{d-1} & \\dots & (x^{(N-1)})^{1}& (x^{(N-1)})^{0}\\\\\n",
    "      (x^{(N)})^{d-1} & \\dots & (x^{(N)})^{1}& (x^{(N)})^{0}\n",
    "    \\end{bmatrix}.\n",
    "\\label{eq8}\n",
    "\\tag{8}\n",
    "\\end{equation*}\n",
    "\n",
    "Polyonimal regression is then nothing but linear regression with the tranformed feature vectors $\\mathbf{x}^{(i)} = \\phi(x^{(i)})$, with $x^{(i)}$ being the Bitcoin closing price at day $i$. \n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Implement a Python function `feature_mapping(x,d)` which reads in vector of feature values $x^{(i)}$ of dataset $\\mathcal{X}$ and returns the feature matrix \\eqref{eq8} with given degree $d$ (by default $d=2$). \n",
    "2. Implement a Python function `polynomialRegression(x,y,degree=1)` which is based on the following steps \n",
    "    * use `feature_mapping(x,d)` to obtain the feature matrix \\eqref{eq8}\n",
    "    * compute the optimal weight vector `w_opt` by inserting the feature matrix \\eqref{eq8} into (5)\n",
    "    * use weight vector to computed predictions $\\hat{y}$ for the labels $y$\n",
    "    * compute the empirical risk obtained when predicting the labels in the dataset $\\mathcal{X}$ \n",
    "3. Generate a scatter plot of the dataset $\\mathcal{X}$ which includes also the optimal polynomial predictor as a red curve by executing the cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fe0c608870c2d7b0d8d852f6acef4fa",
     "grade": false,
     "grade_id": "cell-33866d3d90888e2e",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Linear regression model for feature mapping.\n",
    "def polynomialRegression(x, y, degree=1):\n",
    "    ### STUDENT TASK ###\n",
    "    ## Calculate w_opt, empirical_error, X and Y\n",
    "    # X = ...\n",
    "    # Y = ...\n",
    "    # w_opt=...\n",
    "    # empirical_error=...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return w_opt, empirical_error\n",
    "\n",
    "# Extract feature to higher dimensional by computing feature mapping\n",
    "# return numpy.ndarray with following mappings:\n",
    "#      [[x_1^(d), x_1^(d-1), x_1^(d-2), ... , x_1^(0)]\n",
    "#        ...         ...        ...     ...     ... \n",
    "#       [x_N^(d), x_N^(d-1), x_N^(d-2), ... ,x_N^(0)]]\n",
    "def feature_mapping(x, degree=1):\n",
    "    ### STUDENT TASK ####\n",
    "    ## Compute specified feature mapping by replacing '...' with your solution.\n",
    "    ## Hints! Use x to get all the feature vectors of the data set.\n",
    "    ##        Check out numpy's vstack(), hstack() and column_stack() functions.\n",
    "    # polynomial_features = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return polynomial_features\n",
    "\n",
    "def draw_plot(x, y, title='', degree=1):\n",
    "    w_opt, empirical_error = polynomialRegression(x, y, degree)\n",
    "    \n",
    "    # Change feature values into continues one from 0 to 1.\n",
    "    x_pred = np.linspace(0,1.1,100)\n",
    "    \n",
    "    # predict new y values using feature mapping\n",
    "    y_pred = predict(feature_mapping(x_pred, degree), w_opt)\n",
    "\n",
    "    # Plot data points and linear regression fitting line\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x_pred,y_pred,'r', label=(\"Empirical = %.4f\" % empirical_error))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Bitcoin')\n",
    "    plt.ylabel('Ethereum')\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "######### Linear regression model for x and y data #########\n",
    "draw_plot(x,y,r'$\\bf{Figure 5.}$Normalized cryptocurrency prices',degree=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d93d439b381a13ec486919251c0e9c21",
     "grade": true,
     "grade_id": "cell-4de426f9105b1b2d",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "w_opt, empirical_error = polynomialRegression(x, y)\n",
    "assert empirical_error < 0.01\n",
    "w_opt, empirical_error = polynomialRegression([0,1,2,3], [0,1,2,3])\n",
    "# Because of computational rounding errors, empirical error is almost never 0\n",
    "assert empirical_error < 1e-30\n",
    "for i in range(0,100):\n",
    "    x_test = feature_mapping([0,1,2,3],i)\n",
    "    assert x_test.shape == (4,1+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a318203786335a50759896f6ee3150f",
     "grade": false,
     "grade_id": "cell-f0d489178074a2da",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3.5 Gradient Descent\n",
    "\n",
    "Recall `3.3 Linear Regression`, where the optimal predictor $h^{(\\mathbf{w}_{\\rm opt})}$ was found by minimizing empirical error $\\mathcal{E}(\\mathbf{w})$ using the closed-form solution \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w}_{\\rm opt} = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y},\n",
    "\\label{eq5}\n",
    "\\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "with the matrix $\\mathbf{X}$ containing the feature vectors $\\mathbf{x}^{(i)}$ and the vector $\\mathbf{y}$ containing \n",
    "the labels $y^{(i)}$ of the data points. There are two challenges with using the closed-form expression \\eqref{eq5} to get the optimal weight vector $\\mathbf{w}_{\\rm opt}$: First, it only applies if the matrix $(\\mathbf{X}^T \\mathbf{X})$ is invertible (note that we have little control over the features as those are obtained from measurements). Second, inverting the matrix $\\mathbf{X}^T \\mathbf{X}$ might be computationally infeasible if we have a lot (billions) of features. Luckily, there is an alternative method for finding the optimal weight vector $\\mathbf{w}_{\\rm opt}$ which avoids these two challenges. In particular, this method is based on a very simple method for finding the minimum of a function: gradient descent **(GD)**. \n",
    "\n",
    "GD amounts to iteratively updating a current guess (or approximation) $\\mathbf{w}^{(k)}$ for the optimal weight (which minimizes ([2](#mjx-eqn-eq2))) according to the rule:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - \\alpha {\\nabla}_{\\mathbf{w}}\\mathcal{E} (\\mathbf{w}^{(k)}).\n",
    "    \\label{eq10}\n",
    "    \\tag{10}\n",
    "\\end{equation*}\n",
    "The choice of the step-size $\\alpha$ is crucial for how quickly (and if at all) the iterates $\\mathbf{w}^{(k)}$ converge to an optimal weight vector $\\mathbf{w}_{opt}$ which yields smallest empirical risk (see `3.3 Linear Regression`). \n",
    "\n",
    "Note that the implementation of GD requires to compute the gradient ${\\nabla}_{\\mathbf{w}}\\mathcal{E} (\\mathbf{w})$.\n",
    "For linear regression and squared error loss this gradient is obtained as \n",
    "\n",
    "\\begin{equation*}\n",
    "{\\nabla}_{\\mathbf{w}}\\mathcal{E} (\\mathbf{w}) =-\\frac{2}{N}\\sum^{N}_{i=1}\\mathbf{x}^{(i)}(y^{(i)} - {{\\mathbf{w}}}^{T} \\mathbf{x}^{(i)}) = -\\frac{2}{N} \\mathbf{X}^{T}(\\mathbf{y}-\\mathbf{X} \\mathbf{w}).\n",
    "\\label{eq11}\n",
    "\\tag{11}\n",
    "\\end{equation*}\n",
    "\n",
    "GD is started by initalizing the weight vector $\\mathbf{w}$ to some value, either randomly or setting $\\mathbf{w}^{(0)}=\\big(0,\\dots,0\\big)^{T}$ and then repeating the GD update \\eqref{eq10} until some termination criterion is met. Two simple options for the termination criterion are: (i) using a fixed number of iterations or (ii) checking the decrease of the objective function and stopping when the decrease is below a threshold. \n",
    "\n",
    "It can shown that for a sufficiently small step size $\\alpha$, GD always converges to the optimal weight vector $\\mathbf{w}_{\\rm opt}$, regardless of which inital guess $w^{(0)}$ has been used. The convergence speed depends crucially on the precise choice of the step size $\\alpha$: If $\\alpha$ is too large, the iterations may not converge at all while if $\\alpha$ is too small, the convergence is unacceptably slow. \n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Implement a Python function `gradient()` which computes the gradient according to (Eq. \\ref{eq11}). This Python function requires as input a feature matrix $X$, label vector $y$ and returns a vector.\n",
    "2. Complete Python function `gradient_descent()` which implements the GD update (Eq. \\ref{eq10}). This Python function takes feature matrix $X$, label vector $y$ learning rate $\\alpha$ and max iterations as inputs and outputs an array of empirical errors.\n",
    "3. The code snippet below creates a figure entitled \"$\\bf{Figure\\ 6.}$ Converge of GD\" which depicts the convergence of GD for different values of the step-size (these different values are in variable `learning_rates`\n",
    "4. Determine which learning rate yields fastest convergence and store in the variable `best_alpha` \n",
    "5. Create a $\\bf{Figure\\ 7.}$ which is identical fo $\\bf{Figure\\ 6.}$ except for the color coding of the curves (each curve corrsponds to one parituclar step size): the curve corresponding to `best_alpha` should be red and the remaining curves in blue. \n",
    "\n",
    "### Tips\n",
    "* `empirical_risk()` and `X`, `Y` -variables are same as in `linearRegression()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe4b8041439c7541fc5f83715d5518bb",
     "grade": false,
     "grade_id": "cell-442d318e6198f794",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate empirical error of the prediction\n",
    "def empirical_risk(X, Y, w_opt):\n",
    "    # Assign current weight vector according to iterative weight vector\n",
    "    ### STUDENT TASK ###\n",
    "    ## Compute empirical error by replacing '...' with your solution.\n",
    "    ## Hints! Use X, Y and w_opt to get necessary matrices.\n",
    "    ##        Check out numpy's dot(), mean(), power() and subtract() functions.\n",
    "    # empirical_error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return empirical_error\n",
    "\n",
    "# Compute gradient\n",
    "# return numpy.ndarray:\n",
    "#      [ [w_1]\n",
    "#          ....\n",
    "#        [w_d]]\n",
    "def gradient(X,Y,w):\n",
    "    N = X.shape[0]\n",
    "    ### STUDENT TASK ###\n",
    "    ## Compute gradient by replacing '...' with your solution.\n",
    "    ## Hint! Use X and Y to get necessary matrices.\n",
    "    # gradient = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return gradient\n",
    "\n",
    "# Run GD for k steps\n",
    "# a = alpha/learning rate\n",
    "# k = iteration steps\n",
    "# returns empirical error array with lenght k: [ Empirical_1, ..., Empirical_k ]\n",
    "def gradient_descent(x, y, a, k):\n",
    "    ### STUDENT TASK ###\n",
    "    ## Hints! Same as in linearRegression()!\n",
    "    # X = ...\n",
    "    # Y = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Initial weigth vector (all values 0)\n",
    "    w = np.zeros((X.shape[1], Y.shape[1]))\n",
    "    empirical_errors = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Calculate gradient\n",
    "        grad = gradient(X,Y,w)\n",
    "\n",
    "        ### STUDENT TASK ###\n",
    "        ## Update weight vector by replacing '...' with your solution.\n",
    "        # w = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        ### STUDENT TASK ###\n",
    "        # Calculate Empirical Risk and append the error into empirical_errors array\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return empirical_errors\n",
    "    \n",
    "def visualize_error(X, y, learning_rates, best_alpha = None):\n",
    "    fig, axes = plt.subplots(1, 2,figsize=(12, 4))\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        # Plot Error against Step Size\n",
    "        GD_converge=gradient_descent(x,y,learning_rate, 1000)   \n",
    "        axes[0].plot(GD_converge,label=(r'$\\alpha=$'+str(learning_rate))) \n",
    "\n",
    "    axes[0].set_xlabel('Number of Iterations')\n",
    "    axes[0].set_ylabel('Empirical Error')\n",
    "    axes[0].legend(loc=0)\n",
    "    axes[0].set_title(r'$\\bf{Figure\\ 6.}$Converge of GD')\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        # Plot Error against Step Size.\n",
    "        # Now mark the best converge in red. Use value from best as a correct step size.\n",
    "        GD_converge=gradient_descent(x,y,learning_rate, 1000)\n",
    "\n",
    "        if learning_rate == best_alpha:\n",
    "            axes[1].plot(GD_converge,label=(r'$\\alpha=$'+str(learning_rate)), color=\"red\")\n",
    "        else:\n",
    "            axes[1].plot(GD_converge,label=(r'$\\alpha=$'+str(learning_rate)), color=\"blue\")\n",
    "    \n",
    "    axes[1].set_xlabel('Number of Iterations')\n",
    "    axes[1].set_ylabel('Empirical Error')\n",
    "    axes[1].legend(loc=0)\n",
    "    axes[1].set_title(r'$\\bf{Figure\\ 7.}$Converge of GD')\n",
    "    plt.tight_layout()\n",
    "    return axes, best_alpha\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.02, 0.1, 0.2, 0.3, 0.9, 0.98]\n",
    "\n",
    "### STUDENT TASK ###\n",
    "# Change best=None into step size from the list that provides the fastest converge. e.g best=1\n",
    "###\n",
    "GD_plots, best = visualize_error(x, y, best_alpha=None, learning_rates=learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0748480e4aa21fcc8378d8dc7ffcf1f7",
     "grade": true,
     "grade_id": "cell-41fe85da9ef892e8",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert best != None, \"You haven't specified the best learning rate\"\n",
    "for i in [1,10,100,200,243]:\n",
    "    res = gradient_descent([0,0.5,1], [0,0.5,1], 1e-5, i)\n",
    "    assert len(res) == i, \"Size of the Error array is incorrect\"\n",
    "    assert np.sum(res)/i < 1, \"Your error is way higher than it should be.\"\n",
    "\n",
    "# Check that plots are rendered.\n",
    "from plotchecker import LinePlotChecker\n",
    "for GD_plot in GD_plots:\n",
    "    pc = LinePlotChecker(GD_plot)\n",
    "    pc.assert_num_lines(8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
