{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4994944eda42dc6616b553125e66a8f2",
     "grade": false,
     "grade_id": "cell-e927f5b226d8820f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 3. Part 2. Hyperparameter search\n",
    "\n",
    "## Learning goals\n",
    "* Practical experience in tuning hyperparameters of neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d35ce4a4160dc766d5dafef6587df43",
     "grade": false,
     "grade_id": "cell-deb06164b659c10e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# Select data directory\n",
    "import os\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    # course_data_dir = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c94a73c61cfb1c981c78c190a04afdd0",
     "grade": false,
     "grade_id": "cell-248e4ecd9a43eb69",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device which you are going to use for training\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "874062d2bb88959790dcf82b1e84f37d",
     "grade": false,
     "grade_id": "cell-839f6b565bcf4fc3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79f6c3886e7e58642ab3557c86554e87",
     "grade": false,
     "grade_id": "cell-2f05b92d3a6c0d86",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Grid search\n",
    "\n",
    "Your first task is to implement grid search in the cell below. You are allowed to use only modules imported in the previos cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e0c175dcc66de17b89f324ff1922a26",
     "grade": false,
     "grade_id": "cell-7f8b0904d5f32a5d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grid_search(*iterables):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      iterables: Each iterable is, e.g., a list (tuple or a numpy arrrays) containing grid values\n",
    "                  for one of the tuned parameter.\n",
    "    \n",
    "    Returns:\n",
    "      An iterator over all combinations of the grid values of the given iterables.\n",
    "      Each object returned by the iterator is a tuple whose i-th element is one of the grid values from the\n",
    "      i-th input iterable.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    res=itertools.product(*iterables)\n",
    "    return res\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ddb43ee66296ff1207cddf895ba0d7e",
     "grade": true,
     "grade_id": "grid_search",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1, 0.4, 0.6)\n",
      "(0.1, 0.4, 0.7)\n",
      "(0.1, 0.4, 0.8)\n",
      "(0.1, 0.5, 0.6)\n",
      "(0.1, 0.5, 0.7)\n",
      "(0.1, 0.5, 0.8)\n",
      "(0.2, 0.4, 0.6)\n",
      "(0.2, 0.4, 0.7)\n",
      "(0.2, 0.4, 0.8)\n",
      "(0.2, 0.5, 0.6)\n",
      "(0.2, 0.5, 0.7)\n",
      "(0.2, 0.5, 0.8)\n",
      "(0.3, 0.4, 0.6)\n",
      "(0.3, 0.4, 0.7)\n",
      "(0.3, 0.4, 0.8)\n",
      "(0.3, 0.5, 0.6)\n",
      "(0.3, 0.5, 0.7)\n",
      "(0.3, 0.5, 0.8)\n"
     ]
    }
   ],
   "source": [
    "# Let's test your implementation\n",
    "param1 = [0.1, 0.2, 0.3]  # Iterable with grid values of parameter 1\n",
    "param2 = [0.4, 0.5]       # Iterable with grid values of parameter 2\n",
    "param3 = [0.6, 0.7, 0.8]  # Iterable with grid values of parameter 3\n",
    "for i in grid_search(param1, param2, param3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91d274926876fcb5616dcf43b264bdf0",
     "grade": false,
     "grade_id": "cell-069f4b347dcccf90",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Random search\n",
    "\n",
    "Your second task is to implement random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddd2d450451c994d86c0bec211e722ef",
     "grade": false,
     "grade_id": "cell-b16dd20ed61841c0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def random_search(n, *param_ranges):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      n (int):      Number of hyperparameter combinations to be generated.\n",
    "      param_ranges: Each of the given arguments must be a list [`low`, `high`] where low\n",
    "                     defines the `lower` and `high` defines the upper boundaries of the sampling interval\n",
    "                     for the corresponding parameter.\n",
    "    Returns:\n",
    "      An iterator over n combinations of the hyperparameters. Each hyperparameter value is drawn uniformly\n",
    "      from interval [low, high] specified by the corresponding input argument.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    final=[]\n",
    "    for _ in range(n):\n",
    "        res=[]\n",
    "        for x in param_ranges:\n",
    "            temp=np.random.uniform(x[0],x[1])\n",
    "            res.append(temp)\n",
    "        final.append(res)\n",
    "    return final\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f010d0f523297d534fc8e031a5295d2",
     "grade": true,
     "grade_id": "random_search",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5723614866967698, 1.7831900502747922, 2.880973986187896]\n",
      "[0.7033398152962316, 1.1320394928903854, 2.7839570616848848]\n",
      "[0.8152075517530848, 1.3470887376211778, 2.5450417022369636]\n",
      "[0.8952885302449294, 1.337435788158539, 2.1305527266209787]\n",
      "[0.8329215566030403, 1.5284113026876627, 2.3594815311356134]\n",
      "[0.3810601593713292, 1.592168335826001, 2.620765595861963]\n",
      "[0.5996647190305149, 1.4753404702185344, 2.765418251266575]\n",
      "[0.3712322297570757, 1.3478563869373816, 2.3238853734705778]\n",
      "[0.27302954652697525, 1.7922574013808967, 2.2945776707414005]\n",
      "[0.706790452333354, 1.2023282119666692, 2.4384633044149187]\n"
     ]
    }
   ],
   "source": [
    "n = 10  # Number of hyperparameter combinations\n",
    "param_range1 = [0.1, 0.9]  # lower and upper boundaries for parameter 1 \n",
    "param_range2 = [1.1, 1.9]  # lower and upper boundaries for parameter 2\n",
    "param_range3 = [2.1, 2.9]  # lower and upper boundaries for parameter 3\n",
    "for i in random_search(n, param_range1, param_range2, param_range3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cde47a7132e04ac2aa27eddb14ab45a",
     "grade": false,
     "grade_id": "cell-319b46cfefe56de9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Hyperparameter search on a small dataset\n",
    "\n",
    "Let us tune the hyperparameters of an MLP network to classify wines from the wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60fd0fc5824fc8536ebed0509c9b8fbf",
     "grade": false,
     "grade_id": "cell-e97f1b93aae4eb89",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from /coursedata/winequality\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_dir = os.path.join(course_data_dir, 'winequality')\n",
    "print('Data loaded from %s' % data_dir)\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.read_csv(os.path.join(data_dir, 'winequality-red.csv'), delimiter=';'),\n",
    "    pd.read_csv(os.path.join(data_dir, 'winequality-white.csv'), delimiter=';')\n",
    "])\n",
    "\n",
    "x = df.loc[:, df.columns != 'quality'].values\n",
    "y = df['quality'].values >= 7  # Convert to a binary classification problem\n",
    "\n",
    "# Split into training, validation and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b9adc8f0ffc40389084cd7a7c44cd20",
     "grade": false,
     "grade_id": "cell-af76993a88f117c0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Scaling to zero mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a7c904c922c7231f66e6ca3716c895e",
     "grade": false,
     "grade_id": "cell-dbbe7f34db4b2bfe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use an MLP with two hidden layers and dropout\n",
    "n_inputs = 11\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes, p=0):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(sizes[0], sizes[1]),\n",
    "            nn.Dropout(p),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(sizes[1], sizes[2]),\n",
    "            nn.Dropout(p),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(sizes[2], sizes[3])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "597762c204e06d85aabd9c64cb356df3",
     "grade": false,
     "grade_id": "cell-f6060297287321f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute accuracy of a trained MLP on a given dataset\n",
    "def compute_accuracy(x_test_scaled, y_test, mlp):\n",
    "    mlp.eval()\n",
    "    x = torch.tensor(x_test_scaled, dtype=torch.float, device=device)\n",
    "    outputs = mlp.forward(x)\n",
    "    logits = outputs.cpu().data.numpy()\n",
    "    pred_test = logits.argmax(axis=1)\n",
    "    test_accuracy = accuracy_score(pred_test, y_test)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e7b69d8a13a714e7f436e96a468d31b",
     "grade": false,
     "grade_id": "cell-d128292281c38c47",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Training procedure\n",
    "def train(x_train_scaled, y_train, mlp, lrate, print_every):\n",
    "    optimizer = torch.optim.Adam(mlp.parameters(), lr=lrate)\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma=0.95)\n",
    "    \n",
    "    n_epochs = 1000\n",
    "\n",
    "    train_accuracy_history = []\n",
    "    val_accuracy_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        mlp.train()\n",
    "        scheduler.step()\n",
    "        x = torch.tensor(x_train_scaled, device=device, dtype=torch.float)\n",
    "        y = torch.tensor(y_train.astype(int), device=device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.cross_entropy(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch % print_every) == 0:\n",
    "            # Store the progress of training\n",
    "            with torch.no_grad():\n",
    "                logits = outputs.cpu().data.numpy()\n",
    "                pred_train = logits.argmax(axis=1)\n",
    "                train_accuracy = accuracy_score(pred_train, y_train)\n",
    "                train_accuracy_history.append(train_accuracy)\n",
    "                \n",
    "                # Compute validation accuracy\n",
    "                val_accuracy = compute_accuracy(x_val_scaled, y_val, mlp)\n",
    "                val_accuracy_history.append(val_accuracy)\n",
    "                print('Train Epoch {}: Loss: {:.6f} Train accuracy {:.2f} Valdation accuracy {:.2f}'.format(\n",
    "                    epoch, loss.item(), train_accuracy, val_accuracy))\n",
    "    \n",
    "    return mlp, train_accuracy_history, val_accuracy_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6862ce1391cc94c3fee62e0e0738cff",
     "grade": false,
     "grade_id": "cell-a746f8a12e118f4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let us tune the hyperparameters using our own implementation of random search. Try at least 10 parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbcb5acc5522003c859c9547ff37e766",
     "grade": false,
     "grade_id": "cell-ab668c4f56afd61e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:  [358, 97, 0.011603589266408545, 0.0026818396950994772]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=358, bias=True)\n",
      "    (1): Dropout(p=0.002681839695099477)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=358, out_features=97, bias=True)\n",
      "    (4): Dropout(p=0.002681839695099477)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=97, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.673724 Train accuracy 0.61 Valdation accuracy 0.78\n",
      "Train Epoch 199: Loss: 0.076021 Train accuracy 0.98 Valdation accuracy 0.87\n",
      "Train Epoch 398: Loss: 0.015256 Train accuracy 1.00 Valdation accuracy 0.88\n",
      "Train Epoch 597: Loss: 0.007978 Train accuracy 1.00 Valdation accuracy 0.88\n",
      "Train Epoch 796: Loss: 0.006248 Train accuracy 1.00 Valdation accuracy 0.88\n",
      "Train Epoch 995: Loss: 0.004436 Train accuracy 1.00 Valdation accuracy 0.88\n",
      "Final accuracy: 0.878733031674\n",
      "Hyperparameters:  [165, 52, 0.071157014678774125, 0.019547285709679809]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=165, bias=True)\n",
      "    (1): Dropout(p=0.01954728570967981)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=165, out_features=52, bias=True)\n",
      "    (4): Dropout(p=0.01954728570967981)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=52, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.669124 Train accuracy 0.66 Valdation accuracy 0.79\n",
      "Train Epoch 199: Loss: 0.195437 Train accuracy 0.92 Valdation accuracy 0.83\n",
      "Train Epoch 398: Loss: 0.116385 Train accuracy 0.96 Valdation accuracy 0.85\n",
      "Train Epoch 597: Loss: 0.079210 Train accuracy 0.97 Valdation accuracy 0.86\n",
      "Train Epoch 796: Loss: 0.064821 Train accuracy 0.98 Valdation accuracy 0.86\n",
      "Train Epoch 995: Loss: 0.060010 Train accuracy 0.98 Valdation accuracy 0.86\n",
      "Final accuracy: 0.864253393665\n",
      "Hyperparameters:  [252, 152, 0.020394443564753756, 0.0012686617126142384]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=252, bias=True)\n",
      "    (1): Dropout(p=0.0012686617126142384)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=252, out_features=152, bias=True)\n",
      "    (4): Dropout(p=0.0012686617126142384)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=152, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.714763 Train accuracy 0.45 Valdation accuracy 0.78\n",
      "Train Epoch 199: Loss: 0.089724 Train accuracy 0.98 Valdation accuracy 0.86\n",
      "Train Epoch 398: Loss: 0.022606 Train accuracy 1.00 Valdation accuracy 0.87\n",
      "Train Epoch 597: Loss: 0.011662 Train accuracy 1.00 Valdation accuracy 0.87\n",
      "Train Epoch 796: Loss: 0.008434 Train accuracy 1.00 Valdation accuracy 0.87\n",
      "Train Epoch 995: Loss: 0.006852 Train accuracy 1.00 Valdation accuracy 0.88\n",
      "Final accuracy: 0.875113122172\n",
      "Hyperparameters:  [227, 135, 0.073610687110174891, 0.0029456484321235348]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=227, bias=True)\n",
      "    (1): Dropout(p=0.002945648432123535)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=227, out_features=135, bias=True)\n",
      "    (4): Dropout(p=0.002945648432123535)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=135, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.657568 Train accuracy 0.63 Valdation accuracy 0.79\n",
      "Train Epoch 199: Loss: 0.167759 Train accuracy 0.94 Valdation accuracy 0.83\n",
      "Train Epoch 398: Loss: 0.068698 Train accuracy 0.98 Valdation accuracy 0.86\n",
      "Train Epoch 597: Loss: 0.041994 Train accuracy 0.99 Valdation accuracy 0.86\n",
      "Train Epoch 796: Loss: 0.033093 Train accuracy 0.99 Valdation accuracy 0.85\n",
      "Train Epoch 995: Loss: 0.027381 Train accuracy 0.99 Valdation accuracy 0.86\n",
      "Final accuracy: 0.856108597285\n",
      "Hyperparameters:  [222, 187, 0.078048111719857025, 0.026031588561669126]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=222, bias=True)\n",
      "    (1): Dropout(p=0.026031588561669126)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=222, out_features=187, bias=True)\n",
      "    (4): Dropout(p=0.026031588561669126)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=187, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.691794 Train accuracy 0.53 Valdation accuracy 0.78\n",
      "Train Epoch 199: Loss: 0.255475 Train accuracy 0.88 Valdation accuracy 0.83\n",
      "Train Epoch 398: Loss: 0.184308 Train accuracy 0.92 Valdation accuracy 0.84\n",
      "Train Epoch 597: Loss: 0.145583 Train accuracy 0.94 Valdation accuracy 0.84\n",
      "Train Epoch 796: Loss: 0.135610 Train accuracy 0.94 Valdation accuracy 0.84\n",
      "Train Epoch 995: Loss: 0.125595 Train accuracy 0.95 Valdation accuracy 0.84\n",
      "Final accuracy: 0.837104072398\n",
      "Hyperparameters:  [83, 286, 0.0011913822007732099, 0.0090528148342270721]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=83, bias=True)\n",
      "    (1): Dropout(p=0.009052814834227072)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=83, out_features=286, bias=True)\n",
      "    (4): Dropout(p=0.009052814834227072)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=286, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.700486 Train accuracy 0.48 Valdation accuracy 0.73\n",
      "Train Epoch 199: Loss: 0.333163 Train accuracy 0.84 Valdation accuracy 0.81\n",
      "Train Epoch 398: Loss: 0.308514 Train accuracy 0.86 Valdation accuracy 0.83\n",
      "Train Epoch 597: Loss: 0.285396 Train accuracy 0.87 Valdation accuracy 0.83\n",
      "Train Epoch 796: Loss: 0.271342 Train accuracy 0.87 Valdation accuracy 0.84\n",
      "Train Epoch 995: Loss: 0.260936 Train accuracy 0.88 Valdation accuracy 0.84\n",
      "Final accuracy: 0.838009049774\n",
      "Hyperparameters:  [345, 259, 0.0035241848486917962, 0.073950586536356569]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=345, bias=True)\n",
      "    (1): Dropout(p=0.07395058653635657)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=345, out_features=259, bias=True)\n",
      "    (4): Dropout(p=0.07395058653635657)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=259, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.651909 Train accuracy 0.64 Valdation accuracy 0.79\n",
      "Train Epoch 199: Loss: 0.197490 Train accuracy 0.92 Valdation accuracy 0.85\n",
      "Train Epoch 398: Loss: 0.087795 Train accuracy 0.97 Valdation accuracy 0.87\n",
      "Train Epoch 597: Loss: 0.056638 Train accuracy 0.98 Valdation accuracy 0.87\n",
      "Train Epoch 796: Loss: 0.046727 Train accuracy 0.98 Valdation accuracy 0.87\n",
      "Train Epoch 995: Loss: 0.035115 Train accuracy 0.99 Valdation accuracy 0.87\n",
      "Final accuracy: 0.867873303167\n",
      "Hyperparameters:  [47, 255, 0.0013463976804835903, 0.014811108314358523]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=47, bias=True)\n",
      "    (1): Dropout(p=0.014811108314358523)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=47, out_features=255, bias=True)\n",
      "    (4): Dropout(p=0.014811108314358523)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=255, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.734618 Train accuracy 0.45 Valdation accuracy 0.61\n",
      "Train Epoch 199: Loss: 0.353292 Train accuracy 0.84 Valdation accuracy 0.81\n",
      "Train Epoch 398: Loss: 0.330011 Train accuracy 0.85 Valdation accuracy 0.82\n",
      "Train Epoch 597: Loss: 0.321348 Train accuracy 0.85 Valdation accuracy 0.82\n",
      "Train Epoch 796: Loss: 0.310821 Train accuracy 0.86 Valdation accuracy 0.82\n",
      "Train Epoch 995: Loss: 0.304900 Train accuracy 0.86 Valdation accuracy 0.82\n",
      "Final accuracy: 0.820814479638\n",
      "Hyperparameters:  [222, 73, 0.01257100941801483, 0.035873837749140512]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=222, bias=True)\n",
      "    (1): Dropout(p=0.03587383774914051)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=222, out_features=73, bias=True)\n",
      "    (4): Dropout(p=0.03587383774914051)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=73, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.708314 Train accuracy 0.45 Valdation accuracy 0.77\n",
      "Train Epoch 199: Loss: 0.127856 Train accuracy 0.95 Valdation accuracy 0.87\n",
      "Train Epoch 398: Loss: 0.047505 Train accuracy 0.98 Valdation accuracy 0.87\n",
      "Train Epoch 597: Loss: 0.031200 Train accuracy 0.99 Valdation accuracy 0.88\n",
      "Train Epoch 796: Loss: 0.025365 Train accuracy 0.99 Valdation accuracy 0.88\n",
      "Train Epoch 995: Loss: 0.017579 Train accuracy 0.99 Valdation accuracy 0.87\n",
      "Final accuracy: 0.874208144796\n",
      "Hyperparameters:  [83, 108, 0.0011417648090656367, 0.0091883882734731873]\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=83, bias=True)\n",
      "    (1): Dropout(p=0.009188388273473187)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=83, out_features=108, bias=True)\n",
      "    (4): Dropout(p=0.009188388273473187)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=108, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Train Epoch 0: Loss: 0.703351 Train accuracy 0.44 Valdation accuracy 0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 199: Loss: 0.350688 Train accuracy 0.84 Valdation accuracy 0.81\n",
      "Train Epoch 398: Loss: 0.324472 Train accuracy 0.85 Valdation accuracy 0.82\n",
      "Train Epoch 597: Loss: 0.311339 Train accuracy 0.86 Valdation accuracy 0.82\n",
      "Train Epoch 796: Loss: 0.303888 Train accuracy 0.86 Valdation accuracy 0.83\n",
      "Train Epoch 995: Loss: 0.298314 Train accuracy 0.86 Valdation accuracy 0.83\n",
      "Final accuracy: 0.828054298643\n"
     ]
    }
   ],
   "source": [
    "n = 10  # Number of parameter combinations\n",
    "\n",
    "n_hidden1_range = [10, 400]\n",
    "h_hidden2_range = [10, 400]\n",
    "log_lrate_range = [np.log(0.001), np.log(0.1)]\n",
    "log_dropout_range = [np.log(0.001), np.log(0.3)]\n",
    "\n",
    "hyperparameters = []\n",
    "accuracies = []\n",
    "if not skip_training:\n",
    "    for (n_hidden1, n_hidden2, log_lrate, log_dropout) in \\\n",
    "            random_search(n, n_hidden1_range, h_hidden2_range, log_lrate_range, log_dropout_range):\n",
    "        n_hidden1, n_hidden2 = int(n_hidden1), int(n_hidden2)\n",
    "        lrate, dropout = np.exp(log_lrate), np.exp(log_dropout)\n",
    "        hyperparameters.append([n_hidden1, n_hidden2, lrate, dropout])\n",
    "        print('Hyperparameters: ', hyperparameters[-1])\n",
    "        mlp = MLP([n_inputs, n_hidden1, n_hidden2, 2], p=dropout)\n",
    "        print(mlp)\n",
    "        mlp.to(device)\n",
    "        mlp, train_accuracy_history, val_accuracy_history = train(x_train_scaled, y_train, mlp, lrate, print_every=199)\n",
    "        accuracies.append(val_accuracy_history[-1])\n",
    "        print('Final accuracy:', accuracies[-1])\n",
    "        #print(compute_accuracy(x_test_scaled, y_test, mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5c94a46ba3eff97234d62e87660aa57",
     "grade": false,
     "grade_id": "cell-40620bbbccd150cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters = np.array(hyperparameters)\n",
    "accuracies = np.array(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fea65d13825bc256902124dae4f093ad",
     "grade": true,
     "grade_id": "rs_results_save",
     "locked": true,
     "points": 0.001,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the results of hyperparameter search (type yes to confirm)? yes\n",
      "Results saved to 3_random_search.npz\n"
     ]
    }
   ],
   "source": [
    "# Save results to disk. Submit file `3_random_search.npz` together with your notebook.\n",
    "hs_filename = '3_random_search.npz'\n",
    "if not skip_training:\n",
    "    try:\n",
    "        do_save = input('Do you want to save the results of hyperparameter search (type yes to confirm)? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            np.savez(hs_filename,\n",
    "                     hyperparameters=hyperparameters,\n",
    "                     accuracies=accuracies)\n",
    "            print('Results saved to %s' % hs_filename)\n",
    "        else:\n",
    "            print('Results not saved')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=True.')\n",
    "else:\n",
    "    rs = np.load(hs_filename)\n",
    "    hyperparameters = rs['hyperparameters']\n",
    "    accuracies = rs['accuracies']\n",
    "    print('Results loaded from %s' % hs_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bbc62aa874a18826fc908f57165a678",
     "grade": true,
     "grade_id": "cell-9afe65ee25eef472",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#hidden1 #hidden2 lrate dropout accuracy\n",
      "     358       97 0.012   0.003    0.879\n",
      "     252      152 0.020   0.001    0.875\n",
      "     222       73 0.013   0.036    0.874\n",
      "     345      259 0.004   0.074    0.868\n",
      "     165       52 0.071   0.020    0.864\n",
      "     227      135 0.074   0.003    0.856\n",
      "      83      286 0.001   0.009    0.838\n",
      "     222      187 0.078   0.026    0.837\n",
      "      83      108 0.001   0.009    0.828\n",
      "      47      255 0.001   0.015    0.821\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print('#hidden1 #hidden2 lrate dropout accuracy')\n",
    "ix = accuracies.argsort()[-1::-1]\n",
    "for (n_hidden1, n_hidden2, lrate, dropout), accuracy in zip(hyperparameters[ix], accuracies[ix]):\n",
    "    print('%8d %8d %5.3f %7.3f %8.3f' % (n_hidden1, n_hidden2, lrate, dropout, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53fa45df9a06d0987d1b373a50319f5a",
     "grade": false,
     "grade_id": "cell-dc6b17a7edcc98e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Train the network with the best hyperparameters including validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03698ac45dd63a7e17942ee7d8e14e3a",
     "grade": true,
     "grade_id": "rs_results",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best architecture: MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=358, bias=True)\n",
      "    (1): Dropout(p=0.002681839695099477)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=358, out_features=97, bias=True)\n",
      "    (4): Dropout(p=0.002681839695099477)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=97, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Best validataion accuracy: 0.879\n"
     ]
    }
   ],
   "source": [
    "# Select hyperparameters producing the best validation accuracy\n",
    "best_run = accuracies.argmax()\n",
    "n_hidden1, n_hidden2, lrate, dropout = hyperparameters[best_run]\n",
    "sizes = [n_inputs, int(n_hidden1), int(n_hidden2), 2]\n",
    "mlp = MLP(sizes, p=dropout)\n",
    "mlp.to(device)\n",
    "print('Best architecture:', mlp)\n",
    "print('Best validataion accuracy: %.3f' % accuracies[best_run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5a944fb88a8a8685f2ae420fe4c99d9",
     "grade": false,
     "grade_id": "cell-9d49b9f61dd19efa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: Loss: 0.647048 Train accuracy 0.74 Valdation accuracy 0.79\n",
      "Train Epoch 199: Loss: 0.076466 Train accuracy 0.98 Valdation accuracy 0.99\n",
      "Train Epoch 398: Loss: 0.012738 Train accuracy 1.00 Valdation accuracy 1.00\n",
      "Train Epoch 597: Loss: 0.005924 Train accuracy 1.00 Valdation accuracy 1.00\n",
      "Train Epoch 796: Loss: 0.004155 Train accuracy 1.00 Valdation accuracy 1.00\n",
      "Train Epoch 995: Loss: 0.003485 Train accuracy 1.00 Valdation accuracy 1.00\n"
     ]
    }
   ],
   "source": [
    "# Train the network with the best hyperparameters using also validation data\n",
    "if not skip_training:\n",
    "    mlp, train_accuracy_history, val_accuracy_history = train(\n",
    "        np.vstack((x_train_scaled, x_val_scaled)), np.hstack((y_train, y_val)),\n",
    "        mlp, lrate, print_every=199\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39d148c6e023b4ff72ee03a4a93afbeb",
     "grade": false,
     "grade_id": "cell-185608091d95bedf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model? yes\n",
      "Model saved to 3_mlp.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the network to a file, submit this file together with your notebook\n",
    "filename = '3_mlp.pth'\n",
    "if not skip_training:\n",
    "    try:\n",
    "        do_save = input('Do you want to save the model? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            torch.save(mlp.state_dict(), filename)\n",
    "            print('Model saved to %s' % filename)\n",
    "        else:\n",
    "            print('Model not saved')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=False.')\n",
    "else:\n",
    "    mlp.load_state_dict(torch.load(filename, map_location=lambda storage, loc: storage))\n",
    "    mlp.to(device)\n",
    "    print('Model loaded from %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "857542900ad66047fbf6ea6f9da3d054",
     "grade": true,
     "grade_id": "rs_accuracy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the best model: 0.870\n"
     ]
    }
   ],
   "source": [
    "# Test the accuracy of the network trained with the best hyperparameters\n",
    "mlp.eval()\n",
    "test_accuracy = compute_accuracy(x_test_scaled, y_test, mlp)\n",
    "print(\"Test accuracy of the best model: %.3f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e841c5ad9ad25170b79f699a4cb3770",
     "grade": false,
     "grade_id": "cell-6b7ddbac12776f69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The accuracy should be greater than 0.85."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
